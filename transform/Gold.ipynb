{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, udf, year, month, dayofmonth, to_timestamp,from_unixtime,concat_ws,explode,current_timestamp\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "from pyspark.sql import functions as F\n",
        "from datetime import datetime\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "metadata": {},
      "source": [
        "silver_base = \"abfss://silver@datalakeyassin.dfs.core.windows.net/\"\n",
        "control_file_path = silver_base + \"_control/processed_files.txt\"\n",
        "\n",
        "df_github_silver = spark.read.parquet(silver_base + \"github/\")\n",
        "df_stack_silver  = spark.read.parquet(silver_base + \"stackoverflow/\")\n",
        "df_reddit_silver = spark.read.parquet(silver_base + \"reddit/\")\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "outputs": [],
      "metadata": {},
      "source": [
        "processed_files_df = spark.read.text(control_file_path)\n",
        "last_etl_time_str = processed_files_df.collect()[0][0].strip()\n",
        "last_etl_time = datetime.strptime(last_etl_time_str, \"%Y-%m-%d %H:%M:%S\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "outputs": [],
      "metadata": {},
      "source": [
        "df_github_silver = df_github_silver.filter(col(\"ingestion_time\") > last_etl_time)\n",
        "df_stack_silver = df_stack_silver.filter(col(\"ingestion_time\") > last_etl_time)\n",
        "df_reddit_silver = df_reddit_silver.filter(col(\"ingestion_time\") > last_etl_time)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [],
      "metadata": {},
      "source": [
        "df_reddit_silver.show(5)\n",
        "df_stack_silver.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [],
      "metadata": {},
      "source": [
        "df_github_gold = df_github_silver.withColumn(\n",
        "    \"engagement_raw\",\n",
        "    F.col(\"stars\") + 2*F.col(\"forks\") + F.col(\"watchers\")\n",
        ")\n",
        "\n",
        "\n",
        "df_reddit_gold = df_reddit_silver.withColumn(\n",
        "    \"engagement_raw\",\n",
        "    F.col(\"score\") +\n",
        "    2*F.col(\"num_comments\") +\n",
        "    3*F.col(\"total_awards\")\n",
        ")\n",
        "\n",
        "df_stack_gold = df_stack_silver.withColumn(\n",
        "    \"engagement_raw\",\n",
        "    F.col(\"score\")*0.5 +\n",
        "    F.col(\"answer_count\")*5 +\n",
        "    F.col(\"favorite_count\")*3 +\n",
        "    F.col(\"view_count\")/1000 \n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [],
      "metadata": {},
      "source": [
        "df_github_gold.select(\"engagement_raw\").show(5)\n",
        "df_reddit_gold.select(\"engagement_raw\").show(5)\n",
        "df_stack_gold.select(\"engagement_raw\").show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [],
      "metadata": {},
      "source": [
        "def normalize(df):\n",
        "    stats = df.agg(\n",
        "        F.min(\"engagement_raw\").alias(\"min_eng\"),\n",
        "        F.max(\"engagement_raw\").alias(\"max_eng\")\n",
        "    ).collect()[0]\n",
        "\n",
        "    min_eng = stats[\"min_eng\"]\n",
        "    max_eng = stats[\"max_eng\"]\n",
        "\n",
        "    return df.withColumn(\n",
        "        \"engagement\",\n",
        "        (F.col(\"engagement_raw\") - min_eng) / (max_eng - min_eng)\n",
        "    )\n",
        "df_github_gold = normalize(df_github_gold)\n",
        "df_reddit_gold = normalize(df_reddit_gold)\n",
        "df_stack_gold = normalize(df_stack_gold)\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [],
      "metadata": {},
      "source": [
        "cols = [\"job\", \"tools\", \"year\", \"month\", \"day\", \"engagement\",\"source\",\"ingestion_time\"]\n",
        "\n",
        "df_github_gold = df_github_gold.select(cols)\n",
        "df_reddit_gold = df_reddit_gold.select(cols)\n",
        "df_stack_gold = df_stack_gold.select(cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [],
      "metadata": {},
      "source": [
        "df_github_gold.show(5)\n",
        "df_reddit_gold.show(5)\n",
        "df_stack_gold.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql import functions as F\n",
        "df = df_github_gold.unionByName(df_reddit_gold).unionByName(df_stack_gold)\n",
        "\n",
        "staging_social = (\n",
        "    df\n",
        "    .withColumn(\"tool\", F.explode(\"tools\")) \n",
        "    .groupBy(\"job\", \"tool\", \"year\", \"month\", \"day\", \"source\")\n",
        "    .agg(\n",
        "        F.sum(\"engagement\").alias(\"job_engagement\"), \n",
        "        F.count(\"*\").alias(\"tool_mentions\"),\n",
        "        F.max(\"ingestion_time\").alias(\"ingestion_time\")       \n",
        "    )\n",
        ")\n",
        "\n",
        "staging_social.show(20)\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "outputs": [],
      "metadata": {},
      "source": [
        "staging_social.write.mode(\"append\").parquet(\n",
        "    \"abfss://gold@datalakeyassin.dfs.core.windows.net/social_popularity/\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "outputs": [],
      "metadata": {},
      "source": [
        "current_etl_time = datetime.utcnow()\n",
        "df_etl_time = spark.createDataFrame(\n",
        "    [(current_etl_time.strftime(\"%Y-%m-%d %H:%M:%S\"),)]\n",
        ")\n",
        "\n",
        "# Overwrite the control file in ADLS\n",
        "df_etl_time.coalesce(1).write.mode(\"overwrite\").text(control_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "language_info": {
      "name": "python"
    }
  }
}