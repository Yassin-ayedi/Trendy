{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 28,
      "outputs": [],
      "metadata": {},
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, udf, year, month, dayofmonth, to_timestamp,from_unixtime,concat_ws,input_file_name\n",
        "from pyspark.sql.types import ArrayType, StringType,TimestampType\n",
        "from pyspark.sql import functions as F\n",
        "from datetime import datetime\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "outputs": [],
      "metadata": {},
      "source": [
        "GITHUB_BLACKLIST = {\n",
        "    \"github\", \"git\", \"bash\", \"shell\", \"sh\",\n",
        "    \"html\", \"css\", \"windows\", \"flow\"\n",
        "}\n",
        "keywords={\n",
        "    'ember.js', 'trello', 'symfony', 'laravel', 'javascript', 'ansible', 'zoom', 'monday.com', 'lisp', 'asp.net core', 'rust', 'groovy', 'terminal', 'asp.net', 'excel', 'objective-c', 'angular', 'C', 'bigquery', 'microstrategy', 'react.js', 'dplyr', 'nuxt.js', 'deno', 'crystal', 'unify', 'atlassian', 'sql server', 'python', 'ssis', 'sap', 'f#', 'ionic', 'vue.js', 'digitalocean', 'redis', 'ocaml', 'nosql', 'shell', 'word', 'gtx', 'clojure', 'scala', 'elasticsearch', 'powerpoint', 'phoenix', 'git', 'unix', 'homebrew', 'dax', 'php', 'vmware', 'elixir', 'suse', 'centos', 'wrike', 'puppet', 'dynamodb', 'couchbase', 'kali', 'smartsheet', 'hugging face', 'mlpack', 'dart', 'pulumi', 'rshiny', 'fastify', 'unreal', 'vue', 'visual basic', 'nuix', 'fastapi', 'kubernetes', 'matlab', 'bitbucket', 'perl', 'keras', 'nltk', 'mattermost', 'sas', 'sql', 'lua', 'capacitor', 'flow', 'airtable', 'apl', 'graphql', 'ubuntu', 'yarn', 'node.js', 'redhat', 'fortran', 'arch', 'notion', 'qt', 'hadoop', 'opencv', 'theano', 'matplotlib', 'db2', 'assembly', 'kafka', 'scikit-learn', 'cassandra', 'gatsby', 'wimi', 'debian', 'mariadb', 'pyspark', 'visualbasic', 'svn', 'no-sql', 'asana', 'mxnet', 'chainer', 'linode', 'workfront', 'gcp', 'sqlserver', 'jquery', 'redshift', 'delphi', 'twilio', 'julia', 'swift', 'unity', 'neo4j', 'spark', 'vba', 'openstack', 'java', 'docker', 'play framework', 'esquisse', 'rocketchat', 'xamarin', 'ssrs', 'haskell', 'next.js', 'R', 'confluence', 'asp.netcore', 'planner', 'qlik', 'terraform', 'bash', 'powershell', 'ggplot2', 'dlib', 'electron', 'ruby on rails', 'blazor', 'angular.js', 'golang', 'spreadsheet', 'ibm cloud', 'gdpr', 'react', 'tidyr', 'microsoft lists', 'mongo', 'ringcentral', 'wire', 'spring', 'flask', 'aws', 'npm', 'c#', 'html', 'css', 'clickup', 'datarobot', 'cobol', 'ovh', 'couchdb', 'shogun', 'splunk', 'sass', 'flutter', 't-sql', 'codecommit', 'symphony', 'chef', 'rubyon rails', 'mlr', 'microsoft teams', 'ruby', 'linux', 'dingtalk', 'pascal', 'msaccess', 'looker', 'gitlab', 'drupal', 'airflow', 'seaborn', 'wsl', 'firestore', 'cognos', 'c++', 'fedora', 'sheets', 'numpy', 'plotly', 'github', 'power bi', 'colocation', 'typescript', 'sharepoint', 'mysql', 'svelte', 'jira', 'heroku', 'postgresql', 'sqlite', 'azure', 'vb.net', 'webex', 'tableau', 'databricks', 'kotlin', 'erlang', 'node', 'pytorch', 'alteryx', 'django', 'solidity', 'mongodb', 'outlook', 'tidyverse', 'windows', 'ms access', 'snowflake', 'selenium', 'slack', 'powerbi', 'cordova', 'huggingface', 'watson', 'jupyter', 'aurora', 'firebase', 'visio', 'express', 'oracle', 'google chat', 'spss', 'macos', 'jenkins', 'tensorflow', 'pandas'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "outputs": [],
      "metadata": {},
      "source": [
        "\n",
        "def extract_tools_udf(text):\n",
        "    found = []\n",
        "    if text:\n",
        "        for kw in keywords:\n",
        "            if len(kw) <= 2:\n",
        "                pattern = r'\\b' + re.escape(kw) + r'\\b'\n",
        "                if re.search(pattern, text):\n",
        "                    found.append(kw)\n",
        "            else:\n",
        "                pattern = r'\\b' + re.escape(kw.lower()) + r'\\b'\n",
        "                if re.search(pattern, text.lower()):\n",
        "                    found.append(kw)\n",
        "    return found\n",
        "\n",
        "extract_tools_spark = udf(extract_tools_udf, ArrayType(StringType()))    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "outputs": [],
      "metadata": {},
      "source": [
        "JOB_MAPPING = {\n",
        "    \"dataanalysis\": \"Data Analyst\",\n",
        "    \"data-analysis\": \"Data Analyst\",\n",
        "    \"datascience\": \"Data Scientist\",\n",
        "    \"data-science\": \"Data Scientist\",\n",
        "    \"dataengineering\": \"Data Engineer\",\n",
        "    \"data-engineering\": \"Data Engineer\",\n",
        "    \"businessanalysis\": \"Business Analyst\",\n",
        "    \"business-analysis\": \"Business Analyst\",\n",
        "    \"machinelearning\": \"Machine Learning Engineer\",\n",
        "    \"machine-learning\": \"Machine Learning Engineer\",\n",
        "    \"MachineLearning\": \"Machine Learning Engineer\",\n",
        "    \"Software Engineer\": \"Software Engineer\",\n",
        "    \"software engineer\": \"Software Engineer\",\n",
        "    \"Cloud Engineer\": \"Cloud Engineer\",\n",
        "    \"cloud-computing\": \"Cloud Engineer\"\n",
        "}\n",
        "\n",
        "def normalize_job(job):\n",
        "    if job is None:\n",
        "        return \"Unknown\"\n",
        "    key = job.lower().strip()\n",
        "    return JOB_MAPPING.get(key, job)\n",
        "\n",
        "normalize_job_udf = udf(normalize_job, StringType())\n",
        "\n",
        "JOB_KEYS = set(JOB_MAPPING.keys())              \n",
        "JOB_VALUES = set(JOB_MAPPING.values())   \n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "outputs": [],
      "metadata": {},
      "source": [
        "def filter_blacklist_udf(tools):\n",
        "    return [tool for tool in tools if tool.lower() not in GITHUB_BLACKLIST]\n",
        "filter_udf = udf(filter_blacklist_udf, ArrayType(StringType()))    \n",
        "\n",
        "def filter_stack_udf(tools):\n",
        "    filtered = []\n",
        "    for t in tools:\n",
        "        if t:\n",
        "            low = t.lower().strip()\n",
        "            if low not in JOB_KEYS and low not in JOB_VALUES:\n",
        "                filtered.append(t)\n",
        "    return filtered\n",
        "\n",
        "filter_stack = udf(filter_stack_udf, ArrayType(StringType()))    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "outputs": [],
      "metadata": {},
      "source": [
        "spark = SparkSession.builder \\\n",
        "    .appName(\"SilverLayerProcessing\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "outputs": [],
      "metadata": {},
      "source": [
        "github_path = \"abfss://bronze@datalakeyassin.dfs.core.windows.net/github/*.json\"\n",
        "stackoverflow_path = \"abfss://bronze@datalakeyassin.dfs.core.windows.net/stackoverflow/*.json\"\n",
        "reddit_path = \"abfss://bronze@datalakeyassin.dfs.core.windows.net/reddit/*.json\"\n",
        "\n",
        "silver_base_path = \"abfss://silver@datalakeyassin.dfs.core.windows.net/\"\n",
        "\n",
        "\n",
        "df_github  = spark.read.option(\"multiline\", \"true\").json(github_path)\n",
        "df_stackoverflow  = spark.read.option(\"multiline\", \"true\").json(stackoverflow_path)\n",
        "df_reddit  = spark.read.option(\"multiline\", \"true\").json(reddit_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "outputs": [],
      "metadata": {},
      "source": [
        "control_file_path = silver_base_path + \"_control/processed_files.txt\"\n",
        "\n",
        "\n",
        "processed_files_df = spark.read.text(control_file_path)\n",
        "last_etl_time_str = processed_files_df.collect()[0][0].strip()\n",
        "last_etl_time = datetime.strptime(last_etl_time_str, \"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "        \n",
        "df_github = df_github.withColumn(\"filename\", input_file_name())   \n",
        "df_stackoverflow = df_stackoverflow.withColumn(\"filename\", input_file_name())  \n",
        "df_reddit = df_reddit.withColumn(\"filename\", input_file_name())   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "outputs": [],
      "metadata": {},
      "source": [
        "def extract_ts(filename):\n",
        "    match = re.search(r'(\\d{4}-\\d{2}-\\d{2}_\\d{2}-\\d{2}-\\d{2})', filename)\n",
        "    if match:\n",
        "        ts_str = match.group(1)\n",
        "        return datetime.strptime(ts_str, \"%Y-%m-%d_%H-%M-%S\")\n",
        "    return None\n",
        "\n",
        "extract_ts_udf = udf(extract_ts, TimestampType())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "outputs": [],
      "metadata": {},
      "source": [
        "df_github = df_github.withColumn(\"ingestion_time\", extract_ts_udf(col(\"filename\")))\\\n",
        "                   .drop(\"filename\")\n",
        "df_stackoverflow = df_stackoverflow.withColumn(\"ingestion_time\", extract_ts_udf(col(\"filename\")))\\\n",
        "                   .drop(\"filename\")\n",
        "df_reddit = df_reddit.withColumn(\"ingestion_time\", extract_ts_udf(col(\"filename\")))\\\n",
        "                   .drop(\"filename\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "outputs": [],
      "metadata": {},
      "source": [
        "df_github = df_github.filter(col(\"ingestion_time\") > last_etl_time)\n",
        "df_stackoverflow = df_stackoverflow.filter(col(\"ingestion_time\") > last_etl_time)\n",
        "df_reddit = df_reddit.filter(col(\"ingestion_time\") > last_etl_time)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "outputs": [],
      "metadata": {},
      "source": [
        "df_github  = df_github.withColumn(\"source\", F.lit(\"github\"))\n",
        "df_reddit  = df_reddit.withColumn(\"source\", F.lit(\"reddit\"))\n",
        "df_stackoverflow   = df_stackoverflow.withColumn(\"source\", F.lit(\"stackoverflow\"))\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "outputs": [],
      "metadata": {},
      "source": [
        "df_github.show(5)\n",
        "df_stackoverflow.show(5)\n",
        "df_reddit.show(5)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "outputs": [],
      "metadata": {},
      "source": [
        "df_github = df_github.withColumn(\"job\", normalize_job_udf(\"job\"))\n",
        "df_reddit = df_reddit.withColumn(\"job\", normalize_job_udf(\"job\"))\n",
        "df_stackoverflow = df_stackoverflow.withColumn(\"job\", normalize_job_udf(\"job\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "outputs": [],
      "metadata": {},
      "source": [
        "df_reddit.select(\"job\").distinct().show()\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "outputs": [],
      "metadata": {},
      "source": [
        "df_stackoverflow = df_stackoverflow.withColumn(\"date_converted\", from_unixtime(col(\"date\"))) \\\n",
        "                   .withColumn(\"year\", year(col(\"date_converted\"))) \\\n",
        "                   .withColumn(\"month\", month(col(\"date_converted\"))) \\\n",
        "                   .withColumn(\"day\", dayofmonth(col(\"date_converted\"))) \\\n",
        "                   .drop(\"date_converted\")\\\n",
        "                   .drop(\"date\")\n",
        "\n",
        "# Reddit\n",
        "df_reddit = df_reddit.withColumn(\"date_converted\", from_unixtime(col(\"date\"))) \\\n",
        "                     .withColumn(\"year\", year(col(\"date_converted\"))) \\\n",
        "                     .withColumn(\"month\", month(col(\"date_converted\"))) \\\n",
        "                     .withColumn(\"day\", dayofmonth(col(\"date_converted\"))) \\\n",
        "                     .drop(\"date_converted\")\\\n",
        "                     .drop(\"date\")\n",
        "\n",
        "# GitHub\n",
        "df_github = df_github.withColumn(\"created_at_dt\", col(\"created_at\").cast(\"timestamp\")) \\\n",
        "                     .withColumn(\"year\", year(col(\"created_at_dt\"))) \\\n",
        "                     .withColumn(\"month\", month(col(\"created_at_dt\"))) \\\n",
        "                     .withColumn(\"day\", dayofmonth(col(\"created_at_dt\"))) \\\n",
        "                     .drop(\"created_at_dt\")\\\n",
        "                     .drop(\"created_at\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "outputs": [],
      "metadata": {},
      "source": [
        "df_github.select(\"year\").distinct().show(5)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "outputs": [],
      "metadata": {},
      "source": [
        "df_reddit = df_reddit.withColumn(\"text_to_parse\", concat_ws(\" \", col(\"title\"), col(\"selftext\")))\\\n",
        "                      .drop(\"title\")\\\n",
        "                      .drop(\"selftext\")\n",
        "                      \n",
        "df_github = df_github.withColumn(\"text_to_parse\", concat_ws(\" \", col(\"description\"), col(\"readme\")))\\\n",
        "                      .drop(\"description\")\\\n",
        "                      .drop(\"readme\")\n",
        "df_stackoverflow = df_stackoverflow.withColumn(\"text_to_parse\", concat_ws(\" \", col(\"title\"), col(\"body\")))\\\n",
        "                      .drop(\"title\")\\\n",
        "                      .drop(\"body\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "source": [
        "df_github = df_github.withColumn(\"tools\", extract_tools_spark(col(\"text_to_parse\")))\n",
        "df_github = df_github.withColumn(\"tools\", filter_udf(col(\"tools\")))\n",
        "df_reddit=df_reddit.withColumn(\"tools\", extract_tools_spark(col(\"text_to_parse\")))\n",
        "df_stackoverflow=df_stackoverflow.withColumn(\"tools\", filter_stack(col(\"tools\")))\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "outputs": [],
      "metadata": {},
      "source": [
        "df_github = df_github.drop(\"text_to_parse\")\\\n",
        "                      .drop(\"url\")\\\n",
        "                      .drop(\"language\")\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "outputs": [],
      "metadata": {},
      "source": [
        "df_github.show(5)\n",
        "df_reddit.show(5)\n",
        "df_stackoverflow.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "outputs": [],
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "github_silver_path = silver_base_path + \"github/\"\n",
        "stackoverflow_silver_path = silver_base_path + \"stackoverflow/\"\n",
        "reddit_silver_path = silver_base_path + \"reddit/\"\n",
        "\n",
        "df_github.write.mode(\"append\").partitionBy(\"year\", \"month\").parquet(github_silver_path)\n",
        "df_stackoverflow.write.mode(\"append\").partitionBy(\"year\", \"month\").parquet(stackoverflow_silver_path)\n",
        "df_reddit.write.mode(\"append\").partitionBy(\"year\", \"month\").parquet(reddit_silver_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "outputs": [],
      "metadata": {},
      "source": [
        "# current_etl_time = datetime.utcnow()\n",
        "# with open(control_file_path, \"w\") as f:\n",
        "#     f.write(current_etl_time.strftime(\"%Y-%m-%d %H:%M:%S\"))"
      ]
    }
  ],
  "metadata": {
    "save_output": true,
    "language_info": {
      "name": "python"
    }
  }
}