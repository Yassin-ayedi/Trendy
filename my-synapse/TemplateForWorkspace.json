{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "my-synapse"
		},
		"my-synapse-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'my-synapse-WorkspaceDefaultSqlServer'",
			"defaultValue": "Integrated Security=False;Encrypt=True;Connection Timeout=30;Data Source=tcp:my-synapse.sql.azuresynapse.net,1433;Initial Catalog=@{linkedService().DBName}"
		},
		"my-synapse-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://datalakeyassin.dfs.core.windows.net"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/Pipeline 1')]",
			"type": "Microsoft.Synapse/workspaces/pipelines",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"activities": [
					{
						"name": "Silver",
						"type": "SynapseNotebook",
						"dependsOn": [],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "Silver",
								"type": "NotebookReference"
							},
							"snapshot": true,
							"conf": {
								"spark.dynamicAllocation.enabled": null,
								"spark.dynamicAllocation.minExecutors": null,
								"spark.dynamicAllocation.maxExecutors": null
							},
							"numExecutors": null
						}
					},
					{
						"name": "Script1",
						"type": "Script",
						"dependsOn": [
							{
								"activity": "Gold",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"linkedServiceName": {
							"referenceName": "my-synapse-WorkspaceDefaultSqlServer",
							"type": "LinkedServiceReference",
							"parameters": {
								"DBName": "trendy"
							}
						},
						"typeProperties": {
							"scripts": [
								{
									"type": "Query",
									"text": "INSERT INTO dim_date (full_date, year, month, day)\nSELECT DISTINCT \n    CAST(CONCAT(year,'-',month,'-',day) AS DATE) AS full_date,\n    year,\n    month,\n    day\nFROM stg_social_popularity st\nWHERE NOT EXISTS (\n    SELECT 1 \n    FROM dim_date d\n    WHERE d.full_date = CAST(CONCAT(st.year,'-',st.month,'-',st.day) AS DATE)\n);\n\n\nINSERT INTO dim_tools (tool_name)\nSELECT DISTINCT tool\nFROM stg_social_popularity st\nWHERE NOT EXISTS (\n    SELECT 1 FROM dim_tools t\n    WHERE t.tool_name = st.tool\n);\n\n\nINSERT INTO dim_domains (domain_name)\nSELECT DISTINCT job\nFROM stg_social_popularity st\nWHERE NOT EXISTS (\n    SELECT 1 FROM dim_domains j\n    WHERE j.domain_name = st.job\n);\n\nINSERT INTO dim_source (source_name)\nSELECT DISTINCT source\nFROM stg_social_popularity st\nWHERE NOT EXISTS (\n    SELECT 1 FROM dim_source s\n    WHERE s.source_name = st.source\n);\n\nINSERT INTO fact_social_popularity (date_id, tool_id, domain_id, source_id, job_engagement, tool_mentions)\nSELECT\n    d.date_id,\n    t.tool_id,\n    j.domain_id,\n    s.source_id,\n    st.job_engagement,\n    st.tool_mentions\nFROM stg_social_popularity st\nJOIN dim_date d\n    ON d.year = st.year AND d.month = st.month AND d.day = st.day\nJOIN dim_tools t\n    ON t.tool_name = st.tool\nJOIN dim_domains j\n    ON j.domain_name = st.job\nJOIN dim_source s\n    ON s.source_name = st.source\nWHERE st.ingestion_time > (SELECT MAX(last_run_time) FROM etl_metadata);"
								}
							],
							"scriptBlockExecutionTimeout": "02:00:00"
						}
					},
					{
						"name": "Gold",
						"type": "SynapseNotebook",
						"dependsOn": [
							{
								"activity": "Silver",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"typeProperties": {
							"notebook": {
								"referenceName": "Gold",
								"type": "NotebookReference"
							},
							"snapshot": true,
							"conf": {
								"spark.dynamicAllocation.enabled": null,
								"spark.dynamicAllocation.minExecutors": null,
								"spark.dynamicAllocation.maxExecutors": null
							},
							"numExecutors": null
						}
					},
					{
						"name": "Script2",
						"type": "Script",
						"dependsOn": [
							{
								"activity": "Script1",
								"dependencyConditions": [
									"Succeeded"
								]
							}
						],
						"policy": {
							"timeout": "0.12:00:00",
							"retry": 0,
							"retryIntervalInSeconds": 30,
							"secureOutput": false,
							"secureInput": false
						},
						"userProperties": [],
						"linkedServiceName": {
							"referenceName": "my-synapse-WorkspaceDefaultSqlServer",
							"type": "LinkedServiceReference",
							"parameters": {
								"DBName": "trendy"
							}
						},
						"typeProperties": {
							"scripts": [
								{
									"type": "Query",
									"text": "UPDATE etl_metadata\nSET last_run_time = CURRENT_TIMESTAMP"
								}
							],
							"scriptBlockExecutionTimeout": "02:00:00"
						}
					}
				],
				"policy": {
					"elapsedTimeMetric": {}
				},
				"annotations": [],
				"lastPublishTime": "2025-12-07T15:50:45Z"
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/notebooks/Silver')]",
				"[concat(variables('workspaceId'), '/linkedServices/my-synapse-WorkspaceDefaultSqlServer')]",
				"[concat(variables('workspaceId'), '/notebooks/Gold')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/my-synapse-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('my-synapse-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/my-synapse-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('my-synapse-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/WorkspaceSystemIdentity')]",
			"type": "Microsoft.Synapse/workspaces/credentials",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "ManagedIdentity",
				"typeProperties": {}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ParquetFormat')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "CREATE EXTERNAL FILE FORMAT ParquetFormat\nWITH (\n    FORMAT_TYPE = PARQUET\n);",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "trendy",
						"poolName": "trendy"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SocialGold')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "CREATE MASTER KEY ENCRYPTION BY PASSWORD = 'Trendy!123';\n\nCREATE DATABASE SCOPED CREDENTIAL MyDatalakeCred\nWITH IDENTITY = 'Managed Identity';\n\nCREATE EXTERNAL DATA SOURCE SocialGold\nWITH (\n    LOCATION = 'abfss://gold@datalakeyassin.dfs.core.windows.net'\n);\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "trendy",
						"poolName": "trendy"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/drop')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "TRUNCATE TABLE fact_social_popularity;\nTRUNCATE TABLE dim_date;\nTRUNCATE TABLE dim_tools;\nTRUNCATE TABLE dim_domains;\nTRUNCATE TABLE dim_source;\n-- If exists\nTRUNCATE TABLE etl_metadata;\nINSERT INTO etl_metadata (last_run_time)\nVALUES ('1900-01-01 00:00:00');",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "trendy",
						"poolName": "trendy"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/insert_to_demand_table')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- Insert into dim_date\n-- INSERT INTO dim_date (full_date, year, month, day)\n-- SELECT DISTINCT \n--     CAST(\n--         CONCAT(\n--             CAST(year AS VARCHAR(4)), '-', \n--             RIGHT('0' + CAST(CAST(month AS INT) AS VARCHAR(2)), 2), '-', \n--             RIGHT('0' + CAST(CAST(day AS INT) AS VARCHAR(2)), 2)\n--         ) AS DATE\n--     ) AS full_date,\n--     year,\n--     month,\n--     day\n-- FROM stg_fact_demand st\n-- WHERE year IS NOT NULL\n--   AND month IS NOT NULL\n--   AND day IS NOT NULL\n--   AND NOT EXISTS (\n--       SELECT 1 \n--       FROM dim_date d\n--       WHERE d.full_date = CAST(\n--           CONCAT(\n--               CAST(st.year AS VARCHAR(4)), '-', \n--               RIGHT('0' + CAST(CAST(st.month AS INT) AS VARCHAR(2)), 2), '-', \n--               RIGHT('0' + CAST(CAST(st.day AS INT) AS VARCHAR(2)), 2)\n--           ) AS DATE\n--       )\n--   );\n\n\n\n-- -- Insert into dim_tools\n-- INSERT INTO dim_tools (tool_name)\n-- SELECT DISTINCT skill\n-- FROM stg_fact_demand st\n-- WHERE NOT EXISTS (\n--     SELECT 1 FROM dim_tools t\n--     WHERE t.tool_name = st.skill\n-- );\n\n-- -- Insert into dim_domains\n-- INSERT INTO dim_domains (domain_name)\n-- SELECT DISTINCT job_name\n-- FROM stg_fact_demand st\n-- WHERE NOT EXISTS (\n--     SELECT 1 FROM dim_domains j\n--     WHERE j.domain_name = st.job_name\n-- );\n\n-- Insert into dim_location\n-- INSERT INTO dim_location (location_name)\n-- SELECT DISTINCT job_location\n-- FROM stg_fact_demand st\n-- WHERE NOT EXISTS (\n--     SELECT 1 FROM dim_location l\n--     WHERE l.location_name = st.job_location\n-- );\n\n-- Insert into fact_demand\n-- INSERT INTO fact_demand (date_id, tool_id, domain_id, location_id, job_posting)\n-- SELECT\n--     d.date_id,\n--     t.tool_id,\n--     j.domain_id,\n--     l.location_id,\n--     st.job_postings\n-- FROM stg_fact_demand st\n-- JOIN dim_date d\n--     ON d.year = st.year AND d.month = st.month AND d.day = st.day\n-- JOIN dim_tools t\n--     ON t.tool_name = st.skill\n-- JOIN dim_domains j\n--     ON j.domain_name = st.job_name\n-- JOIN dim_location l\n--     ON l.location_name = st.job_location;\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "trendy",
						"poolName": "trendy"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/insert_to_pop_table')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "INSERT INTO dim_date (full_date, year, month, day)\nSELECT DISTINCT \n    CAST(CONCAT(year,'-',month,'-',day) AS DATE) AS full_date,\n    year,\n    month,\n    day\nFROM stg_social_popularity st\nWHERE NOT EXISTS (\n    SELECT 1 \n    FROM dim_date d\n    WHERE d.full_date = CAST(CONCAT(st.year,'-',st.month,'-',st.day) AS DATE)\n);\n\n\nINSERT INTO dim_tools (tool_name)\nSELECT DISTINCT tool\nFROM stg_social_popularity st\nWHERE NOT EXISTS (\n    SELECT 1 FROM dim_tools t\n    WHERE t.tool_name = st.tool\n);\n\n\nINSERT INTO dim_domains (domain_name)\nSELECT DISTINCT job\nFROM stg_social_popularity st\nWHERE NOT EXISTS (\n    SELECT 1 FROM dim_domains j\n    WHERE j.domain_name = st.job\n);\n\nINSERT INTO dim_source (source_name)\nSELECT DISTINCT source\nFROM stg_social_popularity st\nWHERE NOT EXISTS (\n    SELECT 1 FROM dim_source s\n    WHERE s.source_name = st.source\n);\n\nINSERT INTO fact_social_popularity (date_id, tool_id, domain_id, source_id, job_engagement, tool_mentions)\nSELECT\n    d.date_id,\n    t.tool_id,\n    j.domain_id,\n    s.source_id,\n    st.job_engagement,\n    st.tool_mentions\nFROM stg_social_popularity st\nJOIN dim_date d\n    ON d.year = st.year AND d.month = st.month AND d.day = st.day\nJOIN dim_tools t\n    ON t.tool_name = st.tool\nJOIN dim_domains j\n    ON j.domain_name = st.job\nJOIN dim_source s\n    ON s.source_name = st.source\nWHERE st.ingestion_time > (SELECT MAX(last_run_time) FROM etl_metadata);\n\nUPDATE etl_metadata\nSET last_run_time = CURRENT_TIMESTAMP\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "trendy",
						"poolName": "trendy"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/log')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- SELECT \n--     date_id,\n--     tool_id,\n--     domain_id,\n--     source_id,\n--     job_engagement,\n--     tool_mentions,\n--     COUNT(*) AS duplicate_count\n-- FROM fact_social_popularity\n-- GROUP BY \n--     date_id,\n--     tool_id,\n--     domain_id,\n--     source_id,\n--     job_engagement,\n--     tool_mentions\n-- HAVING COUNT(*) > 1;\nSELECT * from fact_social_popularity",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "trendy",
						"poolName": "trendy"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/metadata')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- Create ETL metadata table\nCREATE TABLE etl_metadata\n(\n    last_run_time DATETIME\n)\nWITH\n(\n    DISTRIBUTION = ROUND_ROBIN\n);\n\n\nINSERT INTO etl_metadata (last_run_time)\nVALUES ('1900-01-01 00:00:00');\n\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "trendy",
						"poolName": "trendy"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/staging')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "-- CREATE EXTERNAL TABLE stg_social_popularity\n-- (\n--     job NVARCHAR(100),\n--     tool NVARCHAR(100),\n--     year INT,\n--     month INT,\n--     day INT,\n--     source NVARCHAR(50),\n--     job_engagement FLOAT,\n--     tool_mentions INT,\n--     ingestion_time DATETIME\n-- )\n-- WITH (\n--     LOCATION = '/social_popularity/',\n--     DATA_SOURCE = SocialGold,\n--     FILE_FORMAT = ParquetFormat\n-- );\n\n-- CREATE EXTERNAL TABLE stg_fact_demand\n-- (\n--     job_name NVARCHAR(100),\n--     skill NVARCHAR(100),\n--     job_location NVARCHAR(50),\n--     year INT,\n--     month INT,\n--     day INT,\n--     job_postings INT\n-- )\n-- WITH (\n--     LOCATION = '/jobs/',\n--     DATA_SOURCE = SocialGold,\n--     FILE_FORMAT = ParquetFormat\n-- );\n\n-- SELECT * from stg_fact_demand\n-- SELECT *\n-- FROM stg_fact_demand\n-- WHERE ISNUMERIC(year) = 0\n--    OR ISNUMERIC(month) = 0\n--    OR ISNUMERIC(day) = 0;\n\n",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "trendy",
						"poolName": "trendy"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/tables cration')]",
			"type": "Microsoft.Synapse/workspaces/sqlscripts",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"content": {
					"query": "CREATE TABLE dim_date (\n    date_id INT IDENTITY(1,1) NOT NULL,\n    full_date DATE,\n    year INT,\n    month INT,\n    day INT,\n    CONSTRAINT PK_dim_date PRIMARY KEY NONCLUSTERED (date_id) NOT ENFORCED\n);\n\nCREATE TABLE dim_tools (\n    tool_id INT IDENTITY(1,1) NOT NULL,\n    tool_name VARCHAR(255),\n    CONSTRAINT PK_dim_tools PRIMARY KEY NONCLUSTERED (tool_id) NOT ENFORCED \n);\n\nCREATE TABLE dim_domains (\n    domain_id INT IDENTITY(1,1) NOT NULL,\n    domain_name VARCHAR(255),\n    CONSTRAINT PK_dim_domains PRIMARY KEY NONCLUSTERED (domain_id) NOT ENFORCED \n);\n\nCREATE TABLE dim_source (\n    source_id INT IDENTITY(1,1) NOT NULL,\n    source_name VARCHAR(50),\n    CONSTRAINT PK_dim_source PRIMARY KEY NONCLUSTERED (source_id) NOT ENFORCED \n);\n\nCREATE TABLE fact_social_popularity (\n    popularity_id INT IDENTITY(1,1) NOT NULL,\n    date_id INT NOT NULL,    \n    tool_id INT NOT NULL,\n    domain_id INT NOT NULL,\n    source_id INT NOT NULL,\n    job_engagement FLOAT,\n    tool_mentions INT,\n    CONSTRAINT PK_fact_social_popularity PRIMARY KEY NONCLUSTERED (popularity_id) NOT ENFORCED \n);\n\n\nCREATE TABLE dim_location (\n    location_id INT IDENTITY(1,1) NOT NULL,\n    location_name VARCHAR(255),\n    CONSTRAINT PK_dim_location PRIMARY KEY NONCLUSTERED (location_id) NOT ENFORCED \n);\n\nCREATE TABLE fact_demand (\n    demand_id INT IDENTITY(1,1) NOT NULL,\n    date_id INT NOT NULL,    \n    tool_id INT NOT NULL,\n    domain_id INT NOT NULL,\n    location_id INT NOT NULL,\n    job_posting INT,\n    CONSTRAINT PK_fact_demand PRIMARY KEY NONCLUSTERED (demand_id) NOT ENFORCED \n);",
					"metadata": {
						"language": "sql"
					},
					"currentConnection": {
						"databaseName": "trendy",
						"poolName": "trendy"
					},
					"resultLimit": 5000
				},
				"type": "SqlQuery"
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Gold')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SilverSpark",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2",
						"spark.autotune.trackingId": "78bb892a-d68a-4789-82a0-fa419354c961"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/7ce1bb29-7a9b-473d-a49e-a335cd2f0e33/resourceGroups/datawarehouse-rg/providers/Microsoft.Synapse/workspaces/my-synapse/bigDataPools/SilverSpark",
						"name": "SilverSpark",
						"type": "Spark",
						"endpoint": "https://my-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SilverSpark",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.5",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import SparkSession\n",
							"from pyspark.sql.functions import col, udf, year, month, dayofmonth, to_timestamp,from_unixtime,concat_ws,explode,current_timestamp\n",
							"from pyspark.sql.types import ArrayType, StringType\n",
							"from pyspark.sql import functions as F\n",
							"from datetime import datetime\n",
							"import re"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "code",
						"source": [
							"silver_base = \"abfss://silver@datalakeyassin.dfs.core.windows.net/\"\n",
							"control_file_path = silver_base + \"_control/processed_files.txt\"\n",
							"\n",
							"df_github_silver = spark.read.parquet(silver_base + \"github/\")\n",
							"df_stack_silver  = spark.read.parquet(silver_base + \"stackoverflow/\")\n",
							"df_reddit_silver = spark.read.parquet(silver_base + \"reddit/\")\n",
							""
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "code",
						"source": [
							"processed_files_df = spark.read.text(control_file_path)\n",
							"last_etl_time_str = processed_files_df.collect()[0][0].strip()\n",
							"last_etl_time = datetime.strptime(last_etl_time_str, \"%Y-%m-%d %H:%M:%S\")"
						],
						"outputs": [],
						"execution_count": 28
					},
					{
						"cell_type": "code",
						"source": [
							"df_github_silver = df_github_silver.filter(col(\"ingestion_time\") > last_etl_time)\n",
							"df_stack_silver = df_stack_silver.filter(col(\"ingestion_time\") > last_etl_time)\n",
							"df_reddit_silver = df_reddit_silver.filter(col(\"ingestion_time\") > last_etl_time)"
						],
						"outputs": [],
						"execution_count": 29
					},
					{
						"cell_type": "code",
						"source": [
							"df_reddit_silver.show(5)\n",
							"df_stack_silver.show(5)"
						],
						"outputs": [],
						"execution_count": 10
					},
					{
						"cell_type": "code",
						"source": [
							"df_github_gold = df_github_silver.withColumn(\n",
							"    \"engagement_raw\",\n",
							"    F.col(\"stars\") + 2*F.col(\"forks\") + F.col(\"watchers\")\n",
							")\n",
							"\n",
							"\n",
							"df_reddit_gold = df_reddit_silver.withColumn(\n",
							"    \"engagement_raw\",\n",
							"    F.col(\"score\") +\n",
							"    2*F.col(\"num_comments\") +\n",
							"    3*F.col(\"total_awards\")\n",
							")\n",
							"\n",
							"df_stack_gold = df_stack_silver.withColumn(\n",
							"    \"engagement_raw\",\n",
							"    F.col(\"score\")*0.5 +\n",
							"    F.col(\"answer_count\")*5 +\n",
							"    F.col(\"favorite_count\")*3 +\n",
							"    F.col(\"view_count\")/1000 \n",
							")"
						],
						"outputs": [],
						"execution_count": 11
					},
					{
						"cell_type": "code",
						"source": [
							"df_github_gold.select(\"engagement_raw\").show(5)\n",
							"df_reddit_gold.select(\"engagement_raw\").show(5)\n",
							"df_stack_gold.select(\"engagement_raw\").show(5)"
						],
						"outputs": [],
						"execution_count": 12
					},
					{
						"cell_type": "code",
						"source": [
							"def normalize(df):\n",
							"    stats = df.agg(\n",
							"        F.min(\"engagement_raw\").alias(\"min_eng\"),\n",
							"        F.max(\"engagement_raw\").alias(\"max_eng\")\n",
							"    ).collect()[0]\n",
							"\n",
							"    min_eng = stats[\"min_eng\"]\n",
							"    max_eng = stats[\"max_eng\"]\n",
							"\n",
							"    return df.withColumn(\n",
							"        \"engagement\",\n",
							"        (F.col(\"engagement_raw\") - min_eng) / (max_eng - min_eng)\n",
							"    )\n",
							"df_github_gold = normalize(df_github_gold)\n",
							"df_reddit_gold = normalize(df_reddit_gold)\n",
							"df_stack_gold = normalize(df_stack_gold)\n",
							"\n",
							""
						],
						"outputs": [],
						"execution_count": 13
					},
					{
						"cell_type": "code",
						"source": [
							"cols = [\"job\", \"tools\", \"year\", \"month\", \"day\", \"engagement\",\"source\",\"ingestion_time\"]\n",
							"\n",
							"df_github_gold = df_github_gold.select(cols)\n",
							"df_reddit_gold = df_reddit_gold.select(cols)\n",
							"df_stack_gold = df_stack_gold.select(cols)"
						],
						"outputs": [],
						"execution_count": 15
					},
					{
						"cell_type": "code",
						"source": [
							"df_github_gold.show(5)\n",
							"df_reddit_gold.show(5)\n",
							"df_stack_gold.show(5)"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import functions as F\n",
							"df = df_github_gold.unionByName(df_reddit_gold).unionByName(df_stack_gold)\n",
							"\n",
							"staging_social = (\n",
							"    df\n",
							"    .withColumn(\"tool\", F.explode(\"tools\")) \n",
							"    .groupBy(\"job\", \"tool\", \"year\", \"month\", \"day\", \"source\")\n",
							"    .agg(\n",
							"        F.sum(\"engagement\").alias(\"job_engagement\"), \n",
							"        F.count(\"*\").alias(\"tool_mentions\"),\n",
							"        F.max(\"ingestion_time\").alias(\"ingestion_time\")       \n",
							"    )\n",
							")\n",
							"\n",
							"staging_social.show(20)\n",
							"\n",
							""
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "code",
						"source": [
							"staging_social.write.mode(\"append\").parquet(\n",
							"    \"abfss://gold@datalakeyassin.dfs.core.windows.net/social_popularity/\"\n",
							")"
						],
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "code",
						"source": [
							"current_etl_time = datetime.utcnow()\n",
							"df_etl_time = spark.createDataFrame(\n",
							"    [(current_etl_time.strftime(\"%Y-%m-%d %H:%M:%S\"),)]\n",
							")\n",
							"\n",
							"# Overwrite the control file in ADLS\n",
							"df_etl_time.coalesce(1).write.mode(\"overwrite\").text(control_file_path)"
						],
						"outputs": [],
						"execution_count": 25
					},
					{
						"cell_type": "code",
						"source": [
							""
						],
						"outputs": []
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/Silver')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SilverSpark",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"runAsWorkspaceSystemIdentity": false,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "4",
						"spark.autotune.trackingId": "09cc2c3c-6d6f-47f5-9b10-453941298175"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/7ce1bb29-7a9b-473d-a49e-a335cd2f0e33/resourceGroups/datawarehouse-rg/providers/Microsoft.Synapse/workspaces/my-synapse/bigDataPools/SilverSpark",
						"name": "SilverSpark",
						"type": "Spark",
						"endpoint": "https://my-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SilverSpark",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net",
							"authHeader": null
						},
						"sparkVersion": "3.5",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28,
						"extraHeader": null
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"source": [
							"from pyspark.sql import SparkSession\n",
							"from pyspark.sql.functions import col, udf, year, month, dayofmonth, to_timestamp,from_unixtime,concat_ws,input_file_name\n",
							"from pyspark.sql.types import ArrayType, StringType,TimestampType\n",
							"from pyspark.sql import functions as F\n",
							"from datetime import datetime\n",
							"import re"
						],
						"outputs": [],
						"execution_count": 28
					},
					{
						"cell_type": "code",
						"source": [
							"GITHUB_BLACKLIST = {\n",
							"    \"github\", \"git\", \"bash\", \"shell\", \"sh\",\n",
							"    \"html\", \"css\", \"windows\", \"flow\"\n",
							"}\n",
							"keywords={\n",
							"    'ember.js', 'trello', 'symfony', 'laravel', 'javascript', 'ansible', 'zoom', 'monday.com', 'lisp', 'asp.net core', 'rust', 'groovy', 'terminal', 'asp.net', 'excel', 'objective-c', 'angular', 'C', 'bigquery', 'microstrategy', 'react.js', 'dplyr', 'nuxt.js', 'deno', 'crystal', 'unify', 'atlassian', 'sql server', 'python', 'ssis', 'sap', 'f#', 'ionic', 'vue.js', 'digitalocean', 'redis', 'ocaml', 'nosql', 'shell', 'word', 'gtx', 'clojure', 'scala', 'elasticsearch', 'powerpoint', 'phoenix', 'git', 'unix', 'homebrew', 'dax', 'php', 'vmware', 'elixir', 'suse', 'centos', 'wrike', 'puppet', 'dynamodb', 'couchbase', 'kali', 'smartsheet', 'hugging face', 'mlpack', 'dart', 'pulumi', 'rshiny', 'fastify', 'unreal', 'vue', 'visual basic', 'nuix', 'fastapi', 'kubernetes', 'matlab', 'bitbucket', 'perl', 'keras', 'nltk', 'mattermost', 'sas', 'sql', 'lua', 'capacitor', 'flow', 'airtable', 'apl', 'graphql', 'ubuntu', 'yarn', 'node.js', 'redhat', 'fortran', 'arch', 'notion', 'qt', 'hadoop', 'opencv', 'theano', 'matplotlib', 'db2', 'assembly', 'kafka', 'scikit-learn', 'cassandra', 'gatsby', 'wimi', 'debian', 'mariadb', 'pyspark', 'visualbasic', 'svn', 'no-sql', 'asana', 'mxnet', 'chainer', 'linode', 'workfront', 'gcp', 'sqlserver', 'jquery', 'redshift', 'delphi', 'twilio', 'julia', 'swift', 'unity', 'neo4j', 'spark', 'vba', 'openstack', 'java', 'docker', 'play framework', 'esquisse', 'rocketchat', 'xamarin', 'ssrs', 'haskell', 'next.js', 'R', 'confluence', 'asp.netcore', 'planner', 'qlik', 'terraform', 'bash', 'powershell', 'ggplot2', 'dlib', 'electron', 'ruby on rails', 'blazor', 'angular.js', 'golang', 'spreadsheet', 'ibm cloud', 'gdpr', 'react', 'tidyr', 'microsoft lists', 'mongo', 'ringcentral', 'wire', 'spring', 'flask', 'aws', 'npm', 'c#', 'html', 'css', 'clickup', 'datarobot', 'cobol', 'ovh', 'couchdb', 'shogun', 'splunk', 'sass', 'flutter', 't-sql', 'codecommit', 'symphony', 'chef', 'rubyon rails', 'mlr', 'microsoft teams', 'ruby', 'linux', 'dingtalk', 'pascal', 'msaccess', 'looker', 'gitlab', 'drupal', 'airflow', 'seaborn', 'wsl', 'firestore', 'cognos', 'c++', 'fedora', 'sheets', 'numpy', 'plotly', 'github', 'power bi', 'colocation', 'typescript', 'sharepoint', 'mysql', 'svelte', 'jira', 'heroku', 'postgresql', 'sqlite', 'azure', 'vb.net', 'webex', 'tableau', 'databricks', 'kotlin', 'erlang', 'node', 'pytorch', 'alteryx', 'django', 'solidity', 'mongodb', 'outlook', 'tidyverse', 'windows', 'ms access', 'snowflake', 'selenium', 'slack', 'powerbi', 'cordova', 'huggingface', 'watson', 'jupyter', 'aurora', 'firebase', 'visio', 'express', 'oracle', 'google chat', 'spss', 'macos', 'jenkins', 'tensorflow', 'pandas'}"
						],
						"outputs": [],
						"execution_count": 29
					},
					{
						"cell_type": "code",
						"source": [
							"\n",
							"def extract_tools_udf(text):\n",
							"    found = []\n",
							"    if text:\n",
							"        for kw in keywords:\n",
							"            if len(kw) <= 2:\n",
							"                pattern = r'\\b' + re.escape(kw) + r'\\b'\n",
							"                if re.search(pattern, text):\n",
							"                    found.append(kw)\n",
							"            else:\n",
							"                pattern = r'\\b' + re.escape(kw.lower()) + r'\\b'\n",
							"                if re.search(pattern, text.lower()):\n",
							"                    found.append(kw)\n",
							"    return found\n",
							"\n",
							"extract_tools_spark = udf(extract_tools_udf, ArrayType(StringType()))    "
						],
						"outputs": [],
						"execution_count": 30
					},
					{
						"cell_type": "code",
						"source": [
							"JOB_MAPPING = {\n",
							"    \"dataanalysis\": \"Data Analyst\",\n",
							"    \"data-analysis\": \"Data Analyst\",\n",
							"    \"datascience\": \"Data Scientist\",\n",
							"    \"data-science\": \"Data Scientist\",\n",
							"    \"dataengineering\": \"Data Engineer\",\n",
							"    \"data-engineering\": \"Data Engineer\",\n",
							"    \"businessanalysis\": \"Business Analyst\",\n",
							"    \"business-analysis\": \"Business Analyst\",\n",
							"    \"machinelearning\": \"Machine Learning Engineer\",\n",
							"    \"machine-learning\": \"Machine Learning Engineer\",\n",
							"    \"MachineLearning\": \"Machine Learning Engineer\",\n",
							"    \"Software Engineer\": \"Software Engineer\",\n",
							"    \"software engineer\": \"Software Engineer\",\n",
							"    \"Cloud Engineer\": \"Cloud Engineer\",\n",
							"    \"cloud-computing\": \"Cloud Engineer\"\n",
							"}\n",
							"\n",
							"def normalize_job(job):\n",
							"    if job is None:\n",
							"        return \"Unknown\"\n",
							"    key = job.lower().strip()\n",
							"    return JOB_MAPPING.get(key, job)\n",
							"\n",
							"normalize_job_udf = udf(normalize_job, StringType())\n",
							"\n",
							"JOB_KEYS = set(JOB_MAPPING.keys())              \n",
							"JOB_VALUES = set(JOB_MAPPING.values())   \n",
							""
						],
						"outputs": [],
						"execution_count": 31
					},
					{
						"cell_type": "code",
						"source": [
							"def filter_blacklist_udf(tools):\n",
							"    return [tool for tool in tools if tool.lower() not in GITHUB_BLACKLIST]\n",
							"filter_udf = udf(filter_blacklist_udf, ArrayType(StringType()))    \n",
							"\n",
							"def filter_stack_udf(tools):\n",
							"    filtered = []\n",
							"    for t in tools:\n",
							"        if t:\n",
							"            low = t.lower().strip()\n",
							"            if low not in JOB_KEYS and low not in JOB_VALUES:\n",
							"                filtered.append(t)\n",
							"    return filtered\n",
							"\n",
							"filter_stack = udf(filter_stack_udf, ArrayType(StringType()))    "
						],
						"outputs": [],
						"execution_count": 32
					},
					{
						"cell_type": "code",
						"source": [
							"spark = SparkSession.builder \\\n",
							"    .appName(\"SilverLayerProcessing\") \\\n",
							"    .getOrCreate()"
						],
						"outputs": [],
						"execution_count": 33
					},
					{
						"cell_type": "code",
						"source": [
							"github_path = \"abfss://bronze@datalakeyassin.dfs.core.windows.net/github/*.json\"\n",
							"stackoverflow_path = \"abfss://bronze@datalakeyassin.dfs.core.windows.net/stackoverflow/*.json\"\n",
							"reddit_path = \"abfss://bronze@datalakeyassin.dfs.core.windows.net/reddit/*.json\"\n",
							"\n",
							"silver_base_path = \"abfss://silver@datalakeyassin.dfs.core.windows.net/\"\n",
							"\n",
							"\n",
							"df_github  = spark.read.option(\"multiline\", \"true\").json(github_path)\n",
							"df_stackoverflow  = spark.read.option(\"multiline\", \"true\").json(stackoverflow_path)\n",
							"df_reddit  = spark.read.option(\"multiline\", \"true\").json(reddit_path)"
						],
						"outputs": [],
						"execution_count": 34
					},
					{
						"cell_type": "code",
						"source": [
							"control_file_path = silver_base_path + \"_control/processed_files.txt\"\n",
							"\n",
							"\n",
							"processed_files_df = spark.read.text(control_file_path)\n",
							"last_etl_time_str = processed_files_df.collect()[0][0].strip()\n",
							"last_etl_time = datetime.strptime(last_etl_time_str, \"%Y-%m-%d %H:%M:%S\")\n",
							"\n",
							"        \n",
							"df_github = df_github.withColumn(\"filename\", input_file_name())   \n",
							"df_stackoverflow = df_stackoverflow.withColumn(\"filename\", input_file_name())  \n",
							"df_reddit = df_reddit.withColumn(\"filename\", input_file_name())   "
						],
						"outputs": [],
						"execution_count": 35
					},
					{
						"cell_type": "code",
						"source": [
							"def extract_ts(filename):\n",
							"    match = re.search(r'(\\d{4}-\\d{2}-\\d{2}_\\d{2}-\\d{2}-\\d{2})', filename)\n",
							"    if match:\n",
							"        ts_str = match.group(1)\n",
							"        return datetime.strptime(ts_str, \"%Y-%m-%d_%H-%M-%S\")\n",
							"    return None\n",
							"\n",
							"extract_ts_udf = udf(extract_ts, TimestampType())"
						],
						"outputs": [],
						"execution_count": 36
					},
					{
						"cell_type": "code",
						"source": [
							"df_github = df_github.withColumn(\"ingestion_time\", extract_ts_udf(col(\"filename\")))\\\n",
							"                   .drop(\"filename\")\n",
							"df_stackoverflow = df_stackoverflow.withColumn(\"ingestion_time\", extract_ts_udf(col(\"filename\")))\\\n",
							"                   .drop(\"filename\")\n",
							"df_reddit = df_reddit.withColumn(\"ingestion_time\", extract_ts_udf(col(\"filename\")))\\\n",
							"                   .drop(\"filename\")"
						],
						"outputs": [],
						"execution_count": 37
					},
					{
						"cell_type": "code",
						"source": [
							"df_github = df_github.filter(col(\"ingestion_time\") > last_etl_time)\n",
							"df_stackoverflow = df_stackoverflow.filter(col(\"ingestion_time\") > last_etl_time)\n",
							"df_reddit = df_reddit.filter(col(\"ingestion_time\") > last_etl_time)\n",
							""
						],
						"outputs": [],
						"execution_count": 38
					},
					{
						"cell_type": "code",
						"source": [
							"df_github  = df_github.withColumn(\"source\", F.lit(\"github\"))\n",
							"df_reddit  = df_reddit.withColumn(\"source\", F.lit(\"reddit\"))\n",
							"df_stackoverflow   = df_stackoverflow.withColumn(\"source\", F.lit(\"stackoverflow\"))\n",
							""
						],
						"outputs": [],
						"execution_count": 39
					},
					{
						"cell_type": "code",
						"source": [
							"df_github.show(5)\n",
							"df_stackoverflow.show(5)\n",
							"df_reddit.show(5)\n",
							""
						],
						"outputs": [],
						"execution_count": 40
					},
					{
						"cell_type": "code",
						"source": [
							"df_github = df_github.withColumn(\"job\", normalize_job_udf(\"job\"))\n",
							"df_reddit = df_reddit.withColumn(\"job\", normalize_job_udf(\"job\"))\n",
							"df_stackoverflow = df_stackoverflow.withColumn(\"job\", normalize_job_udf(\"job\"))"
						],
						"outputs": [],
						"execution_count": 41
					},
					{
						"cell_type": "code",
						"source": [
							"df_reddit.select(\"job\").distinct().show()\n",
							""
						],
						"outputs": [],
						"execution_count": 42
					},
					{
						"cell_type": "code",
						"source": [
							"df_stackoverflow = df_stackoverflow.withColumn(\"date_converted\", from_unixtime(col(\"date\"))) \\\n",
							"                   .withColumn(\"year\", year(col(\"date_converted\"))) \\\n",
							"                   .withColumn(\"month\", month(col(\"date_converted\"))) \\\n",
							"                   .withColumn(\"day\", dayofmonth(col(\"date_converted\"))) \\\n",
							"                   .drop(\"date_converted\")\\\n",
							"                   .drop(\"date\")\n",
							"\n",
							"# Reddit\n",
							"df_reddit = df_reddit.withColumn(\"date_converted\", from_unixtime(col(\"date\"))) \\\n",
							"                     .withColumn(\"year\", year(col(\"date_converted\"))) \\\n",
							"                     .withColumn(\"month\", month(col(\"date_converted\"))) \\\n",
							"                     .withColumn(\"day\", dayofmonth(col(\"date_converted\"))) \\\n",
							"                     .drop(\"date_converted\")\\\n",
							"                     .drop(\"date\")\n",
							"\n",
							"# GitHub\n",
							"df_github = df_github.withColumn(\"created_at_dt\", col(\"created_at\").cast(\"timestamp\")) \\\n",
							"                     .withColumn(\"year\", year(col(\"created_at_dt\"))) \\\n",
							"                     .withColumn(\"month\", month(col(\"created_at_dt\"))) \\\n",
							"                     .withColumn(\"day\", dayofmonth(col(\"created_at_dt\"))) \\\n",
							"                     .drop(\"created_at_dt\")\\\n",
							"                     .drop(\"created_at\")"
						],
						"outputs": [],
						"execution_count": 43
					},
					{
						"cell_type": "code",
						"source": [
							"df_github.select(\"year\").distinct().show(5)\n",
							""
						],
						"outputs": [],
						"execution_count": 44
					},
					{
						"cell_type": "code",
						"source": [
							"df_reddit = df_reddit.withColumn(\"text_to_parse\", concat_ws(\" \", col(\"title\"), col(\"selftext\")))\\\n",
							"                      .drop(\"title\")\\\n",
							"                      .drop(\"selftext\")\n",
							"                      \n",
							"df_github = df_github.withColumn(\"text_to_parse\", concat_ws(\" \", col(\"description\"), col(\"readme\")))\\\n",
							"                      .drop(\"description\")\\\n",
							"                      .drop(\"readme\")\n",
							"df_stackoverflow = df_stackoverflow.withColumn(\"text_to_parse\", concat_ws(\" \", col(\"title\"), col(\"body\")))\\\n",
							"                      .drop(\"title\")\\\n",
							"                      .drop(\"body\")"
						],
						"outputs": [],
						"execution_count": 45
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"outputs_hidden": true
							}
						},
						"source": [
							"df_github = df_github.withColumn(\"tools\", extract_tools_spark(col(\"text_to_parse\")))\n",
							"df_github = df_github.withColumn(\"tools\", filter_udf(col(\"tools\")))\n",
							"df_reddit=df_reddit.withColumn(\"tools\", extract_tools_spark(col(\"text_to_parse\")))\n",
							"df_stackoverflow=df_stackoverflow.withColumn(\"tools\", filter_stack(col(\"tools\")))\n",
							"\n",
							""
						],
						"outputs": [],
						"execution_count": 46
					},
					{
						"cell_type": "code",
						"source": [
							"df_github = df_github.drop(\"text_to_parse\")\\\n",
							"                      .drop(\"url\")\\\n",
							"                      .drop(\"language\")\n",
							""
						],
						"outputs": [],
						"execution_count": 47
					},
					{
						"cell_type": "code",
						"source": [
							"df_github.show(5)\n",
							"df_reddit.show(5)\n",
							"df_stackoverflow.show(5)"
						],
						"outputs": [],
						"execution_count": 48
					},
					{
						"cell_type": "code",
						"source": [
							"\n",
							"\n",
							"github_silver_path = silver_base_path + \"github/\"\n",
							"stackoverflow_silver_path = silver_base_path + \"stackoverflow/\"\n",
							"reddit_silver_path = silver_base_path + \"reddit/\"\n",
							"\n",
							"df_github.write.mode(\"append\").partitionBy(\"year\", \"month\").parquet(github_silver_path)\n",
							"df_stackoverflow.write.mode(\"append\").partitionBy(\"year\", \"month\").parquet(stackoverflow_silver_path)\n",
							"df_reddit.write.mode(\"append\").partitionBy(\"year\", \"month\").parquet(reddit_silver_path)"
						],
						"outputs": [],
						"execution_count": 52
					},
					{
						"cell_type": "code",
						"source": [
							"# current_etl_time = datetime.utcnow()\n",
							"# with open(control_file_path, \"w\") as f:\n",
							"#     f.write(current_etl_time.strftime(\"%Y-%m-%d %H:%M:%S\"))"
						],
						"outputs": [],
						"execution_count": 50
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/jobs-ETL')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"nbformat": 4,
				"nbformat_minor": 2,
				"bigDataPool": {
					"referenceName": "SilverSpark",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 1,
					"conf": {
						"spark.dynamicAllocation.enabled": "true",
						"spark.dynamicAllocation.minExecutors": "1",
						"spark.dynamicAllocation.maxExecutors": "4",
						"spark.autotune.trackingId": "c28c7730-8579-461e-a21c-7d3434b356db"
					}
				},
				"metadata": {
					"saveOutput": true,
					"enableDebugMode": false,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "python"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/7ce1bb29-7a9b-473d-a49e-a335cd2f0e33/resourceGroups/datawarehouse-rg/providers/Microsoft.Synapse/workspaces/my-synapse/bigDataPools/SilverSpark",
						"name": "SilverSpark",
						"type": "Spark",
						"endpoint": "https://my-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SilverSpark",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.5",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28,
						"automaticScaleJobs": true
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"from pyspark.sql import SparkSession\n",
							"from pyspark.sql.functions import col, to_date,udf\n",
							"from pyspark.sql.types import ArrayType, StringType\n",
							"from pyspark.sql.functions import year, month, dayofmonth\n",
							"from pyspark.sql import functions as F\n",
							"import re"
						],
						"outputs": [],
						"execution_count": 16
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"keywords={\n",
							"    'ember.js', 'trello', 'symfony', 'laravel', 'javascript', 'ansible', 'zoom', 'monday.com', 'lisp', 'asp.net core', 'rust', 'groovy', 'terminal', 'asp.net', 'excel', 'objective-c', 'angular', 'C', 'bigquery', 'microstrategy', 'react.js', 'dplyr', 'nuxt.js', 'deno', 'crystal', 'unify', 'atlassian', 'sql server', 'python', 'ssis', 'sap', 'f#', 'ionic', 'vue.js', 'digitalocean', 'redis', 'ocaml', 'nosql', 'shell', 'word', 'gtx', 'clojure', 'scala', 'elasticsearch', 'powerpoint', 'phoenix', 'git', 'unix', 'homebrew', 'dax', 'php', 'vmware', 'elixir', 'suse', 'centos', 'wrike', 'puppet', 'dynamodb', 'couchbase', 'kali', 'smartsheet', 'hugging face', 'mlpack', 'dart', 'pulumi', 'rshiny', 'fastify', 'unreal', 'vue', 'visual basic', 'nuix', 'fastapi', 'kubernetes', 'matlab', 'bitbucket', 'perl', 'keras', 'nltk', 'mattermost', 'sas', 'sql', 'lua', 'capacitor', 'flow', 'airtable', 'apl', 'graphql', 'ubuntu', 'yarn', 'node.js', 'redhat', 'fortran', 'arch', 'notion', 'qt', 'hadoop', 'opencv', 'theano', 'matplotlib', 'db2', 'assembly', 'kafka', 'scikit-learn', 'cassandra', 'gatsby', 'wimi', 'debian', 'mariadb', 'pyspark', 'visualbasic', 'svn', 'no-sql', 'asana', 'mxnet', 'chainer', 'linode', 'workfront', 'gcp', 'sqlserver', 'jquery', 'redshift', 'delphi', 'twilio', 'julia', 'swift', 'unity', 'neo4j', 'spark', 'vba', 'openstack', 'java', 'docker', 'play framework', 'esquisse', 'rocketchat', 'xamarin', 'ssrs', 'haskell', 'next.js', 'R', 'confluence', 'asp.netcore', 'planner', 'qlik', 'terraform', 'bash', 'powershell', 'ggplot2', 'dlib', 'electron', 'ruby on rails', 'blazor', 'angular.js', 'golang', 'spreadsheet', 'ibm cloud', 'gdpr', 'react', 'tidyr', 'microsoft lists', 'mongo', 'ringcentral', 'wire', 'spring', 'flask', 'aws', 'npm', 'c#', 'html', 'css', 'clickup', 'datarobot', 'cobol', 'ovh', 'couchdb', 'shogun', 'splunk', 'sass', 'flutter', 't-sql', 'codecommit', 'symphony', 'chef', 'rubyon rails', 'mlr', 'microsoft teams', 'ruby', 'linux', 'dingtalk', 'pascal', 'msaccess', 'looker', 'gitlab', 'drupal', 'airflow', 'seaborn', 'wsl', 'firestore', 'cognos', 'c++', 'fedora', 'sheets', 'numpy', 'plotly', 'github', 'power bi', 'colocation', 'typescript', 'sharepoint', 'mysql', 'svelte', 'jira', 'heroku', 'postgresql', 'sqlite', 'azure', 'vb.net', 'webex', 'tableau', 'databricks', 'kotlin', 'erlang', 'node', 'pytorch', 'alteryx', 'django', 'solidity', 'mongodb', 'outlook', 'tidyverse', 'windows', 'ms access', 'snowflake', 'selenium', 'slack', 'powerbi', 'cordova', 'huggingface', 'watson', 'jupyter', 'aurora', 'firebase', 'visio', 'express', 'oracle', 'google chat', 'spss', 'macos', 'jenkins', 'tensorflow', 'pandas'}"
						],
						"outputs": [],
						"execution_count": 17
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"spark = SparkSession.builder.getOrCreate()\n",
							"spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
							"\n",
							""
						],
						"outputs": [],
						"execution_count": 18
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"bronze_jobs_path = \"abfss://bronze@datalakeyassin.dfs.core.windows.net/jobs/\"\n",
							"silver_jobs_path = \"abfss://silver@datalakeyassin.dfs.core.windows.net/jobs/\"\n",
							"gold_jobs_path = \"abfss://gold@datalakeyassin.dfs.core.windows.net/jobs/\""
						],
						"outputs": [],
						"execution_count": 19
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"bronze_jobs_path = \"abfss://bronze@datalakeyassin.dfs.core.windows.net/jobs/\"\n",
							"silver_jobs_path = \"abfss://silver@datalakeyassin.dfs.core.windows.net/jobs/\"\n",
							"\n",
							"\n",
							"columns_map = {\n",
							"    \"data_jobs1.csv\": [\"Role\", \"Location\", \"Posted_Date\", \"Skills\"],\n",
							"    \"data_jobs2.csv\": [\"job_title_short\", \"job_location\", \"job_posted_date\", \"job_skills\"],\n",
							"    \"data_jobs3.csv\": [\"job_title\", \"full_address\", \"job_posted_date\", \"job_description_text\"],\n",
							"    \"data_jobs4.csv\": [\"title\", \"location\", \"job_posted_date\", \"skills\"],\n",
							"    \"data_jobs5.csv\": [\"title\", \"location\", \"posted_date\", \"description\"]\n",
							"}\n",
							"\n",
							"\n",
							"unique_cols = [\"job_title\", \"job_location\", \"job_posted_date\", \"job_skills\"]\n",
							"\n",
							"df_list = []\n",
							"for file_name, cols_to_keep in columns_map.items():\n",
							"    df = spark.read.option(\"header\", True).csv(f\"{bronze_jobs_path}{file_name}\")\n",
							"    df = df.select([col(c) for c in cols_to_keep])\n",
							"\n",
							"    for old_col, new_col in zip(cols_to_keep, unique_cols):\n",
							"        df = df.withColumnRenamed(old_col, new_col)\n",
							"    df_list.append(df)\n",
							"\n",
							"df_jobs = df_list[0]\n",
							"for df in df_list[1:]:\n",
							"    df_jobs = df_jobs.unionByName(df)\n",
							"\n",
							"\n",
							"df_jobs = df_jobs.withColumn(\"job_posted_date\", to_date(col(\"job_posted_date\"), \"yyyy-MM-dd\"))"
						],
						"outputs": [],
						"execution_count": 20
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"def extract_tools(text):\n",
							"    found = []\n",
							"    if text:\n",
							"        for kw in keywords:\n",
							"            if len(kw) <= 2:\n",
							"                pattern = r'\\b' + re.escape(kw) + r'\\b'\n",
							"                if re.search(pattern, text):\n",
							"                    found.append(kw)\n",
							"            else:\n",
							"                pattern = r'\\b' + re.escape(kw.lower()) + r'\\b'\n",
							"                if re.search(pattern, text.lower()):\n",
							"                    found.append(kw)\n",
							"    return found\n",
							"\n",
							"extract_tools_spark_udf = udf(extract_tools, ArrayType(StringType()))"
						],
						"outputs": [],
						"execution_count": 21
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"df_jobs = df_jobs.withColumn(\"tools\", extract_tools_spark_udf(\"job_skills\"))"
						],
						"outputs": [],
						"execution_count": 22
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"it_job_titles = [\n",
							"    \"Data Scientist\", \"Data Analyst\", \"Business Analyst\", \"Data Engineer\",\n",
							"    \"Machine Learning Engineer\", \"ML Ops Engineer\", \"NLP Engineer\", \"Computer Vision Engineer\",\n",
							"    \"Software Engineer\", \"Backend Engineer\", \"Frontend Engineer\", \"Full Stack Engineer\",\n",
							"    \"Mobile App Developer\", \"DevOps Engineer\", \"Cloud Engineer\",\n",
							"    \"Python Developer\", \"Java Developer\", \"PHP Developer\", \"Angular Developer\", \"Vue.js Developer\",\n",
							"    \"Database Developer\", \"ETL Developer\"\n",
							"]\n",
							"\n",
							"def match_job(title):\n",
							"    if not title:\n",
							"        return None\n",
							"    title_lower = title.lower()\n",
							"    for job in it_job_titles:\n",
							"        if job.lower() in title_lower:\n",
							"            return job\n",
							"    return title  # fallback: keep original\n",
							"\n",
							"match_job_udf = udf(match_job, StringType())\n",
							"\n",
							"df_jobs = df_jobs.withColumn(\"job_name\", match_job_udf(\"job_title\"))\n",
							""
						],
						"outputs": [],
						"execution_count": 23
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"\n",
							"\n",
							"df_jobs = df_jobs.withColumn(\"year\", year(\"job_posted_date\")) \\\n",
							"                      .withColumn(\"month\", month(\"job_posted_date\")) \\\n",
							"                      .withColumn(\"day\", dayofmonth(\"job_posted_date\")) \\\n",
							"                      .select(\"job_name\", \"job_location\", \"job_posted_date\", \"tools\", \"year\", \"month\", \"day\")\n",
							""
						],
						"outputs": [],
						"execution_count": 24
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"df_jobs.show(5)"
						],
						"outputs": [],
						"execution_count": 25
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"df_github.write.mode(\"append\").partitionBy(\"year\", \"month\").parquet(silver_jobs_path)"
						],
						"outputs": [],
						"execution_count": 26
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"\n",
							"staging_jobs = (\n",
							"    df_jobs\n",
							"    .withColumn(\"skill\", F.explode(\"tools\"))   \n",
							"    .groupBy(\"job_name\", \"skill\", \"job_location\", \"year\", \"month\", \"day\")\n",
							"    .agg(\n",
							"        F.count(\"*\").alias(\"job_postings\")  # number of postings for this job+skill+location+dat\n",
							"    )\n",
							")\n",
							"\n",
							"staging_jobs.show(10)\n",
							""
						],
						"outputs": [],
						"execution_count": 27
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"staging_social.write.mode(\"append\").parquet(gold_jobs_path)"
						],
						"outputs": [],
						"execution_count": 28
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/SilverSpark')]",
			"type": "Microsoft.Synapse/workspaces/bigDataPools",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"autoPause": {
					"enabled": true,
					"delayInMinutes": 15
				},
				"autoScale": {
					"enabled": true,
					"maxNodeCount": 10,
					"minNodeCount": 3
				},
				"nodeCount": 0,
				"nodeSize": "Small",
				"nodeSizeFamily": "MemoryOptimized",
				"sparkVersion": "3.5",
				"isComputeIsolationEnabled": false,
				"sessionLevelPackagesEnabled": true,
				"annotations": []
			},
			"dependsOn": [],
			"location": "swedencentral"
		}
	]
}