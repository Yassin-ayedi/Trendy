{
	"name": "Silver",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "SilverSpark",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 1,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "true",
				"spark.dynamicAllocation.minExecutors": "1",
				"spark.dynamicAllocation.maxExecutors": "4",
				"spark.autotune.trackingId": "09cc2c3c-6d6f-47f5-9b10-453941298175"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/7ce1bb29-7a9b-473d-a49e-a335cd2f0e33/resourceGroups/datawarehouse-rg/providers/Microsoft.Synapse/workspaces/my-synapse/bigDataPools/SilverSpark",
				"name": "SilverSpark",
				"type": "Spark",
				"endpoint": "https://my-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SilverSpark",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net",
					"authHeader": null
				},
				"sparkVersion": "3.5",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"extraHeader": null
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql import SparkSession\n",
					"from pyspark.sql.functions import col, udf, year, month, dayofmonth, to_timestamp,from_unixtime,concat_ws,input_file_name\n",
					"from pyspark.sql.types import ArrayType, StringType,TimestampType\n",
					"from pyspark.sql import functions as F\n",
					"from datetime import datetime\n",
					"import re"
				],
				"execution_count": 28
			},
			{
				"cell_type": "code",
				"source": [
					"GITHUB_BLACKLIST = {\n",
					"    \"github\", \"git\", \"bash\", \"shell\", \"sh\",\n",
					"    \"html\", \"css\", \"windows\", \"flow\"\n",
					"}\n",
					"keywords={\n",
					"    'ember.js', 'trello', 'symfony', 'laravel', 'javascript', 'ansible', 'zoom', 'monday.com', 'lisp', 'asp.net core', 'rust', 'groovy', 'terminal', 'asp.net', 'excel', 'objective-c', 'angular', 'C', 'bigquery', 'microstrategy', 'react.js', 'dplyr', 'nuxt.js', 'deno', 'crystal', 'unify', 'atlassian', 'sql server', 'python', 'ssis', 'sap', 'f#', 'ionic', 'vue.js', 'digitalocean', 'redis', 'ocaml', 'nosql', 'shell', 'word', 'gtx', 'clojure', 'scala', 'elasticsearch', 'powerpoint', 'phoenix', 'git', 'unix', 'homebrew', 'dax', 'php', 'vmware', 'elixir', 'suse', 'centos', 'wrike', 'puppet', 'dynamodb', 'couchbase', 'kali', 'smartsheet', 'hugging face', 'mlpack', 'dart', 'pulumi', 'rshiny', 'fastify', 'unreal', 'vue', 'visual basic', 'nuix', 'fastapi', 'kubernetes', 'matlab', 'bitbucket', 'perl', 'keras', 'nltk', 'mattermost', 'sas', 'sql', 'lua', 'capacitor', 'flow', 'airtable', 'apl', 'graphql', 'ubuntu', 'yarn', 'node.js', 'redhat', 'fortran', 'arch', 'notion', 'qt', 'hadoop', 'opencv', 'theano', 'matplotlib', 'db2', 'assembly', 'kafka', 'scikit-learn', 'cassandra', 'gatsby', 'wimi', 'debian', 'mariadb', 'pyspark', 'visualbasic', 'svn', 'no-sql', 'asana', 'mxnet', 'chainer', 'linode', 'workfront', 'gcp', 'sqlserver', 'jquery', 'redshift', 'delphi', 'twilio', 'julia', 'swift', 'unity', 'neo4j', 'spark', 'vba', 'openstack', 'java', 'docker', 'play framework', 'esquisse', 'rocketchat', 'xamarin', 'ssrs', 'haskell', 'next.js', 'R', 'confluence', 'asp.netcore', 'planner', 'qlik', 'terraform', 'bash', 'powershell', 'ggplot2', 'dlib', 'electron', 'ruby on rails', 'blazor', 'angular.js', 'golang', 'spreadsheet', 'ibm cloud', 'gdpr', 'react', 'tidyr', 'microsoft lists', 'mongo', 'ringcentral', 'wire', 'spring', 'flask', 'aws', 'npm', 'c#', 'html', 'css', 'clickup', 'datarobot', 'cobol', 'ovh', 'couchdb', 'shogun', 'splunk', 'sass', 'flutter', 't-sql', 'codecommit', 'symphony', 'chef', 'rubyon rails', 'mlr', 'microsoft teams', 'ruby', 'linux', 'dingtalk', 'pascal', 'msaccess', 'looker', 'gitlab', 'drupal', 'airflow', 'seaborn', 'wsl', 'firestore', 'cognos', 'c++', 'fedora', 'sheets', 'numpy', 'plotly', 'github', 'power bi', 'colocation', 'typescript', 'sharepoint', 'mysql', 'svelte', 'jira', 'heroku', 'postgresql', 'sqlite', 'azure', 'vb.net', 'webex', 'tableau', 'databricks', 'kotlin', 'erlang', 'node', 'pytorch', 'alteryx', 'django', 'solidity', 'mongodb', 'outlook', 'tidyverse', 'windows', 'ms access', 'snowflake', 'selenium', 'slack', 'powerbi', 'cordova', 'huggingface', 'watson', 'jupyter', 'aurora', 'firebase', 'visio', 'express', 'oracle', 'google chat', 'spss', 'macos', 'jenkins', 'tensorflow', 'pandas'}"
				],
				"execution_count": 29
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"def extract_tools_udf(text):\n",
					"    found = []\n",
					"    if text:\n",
					"        for kw in keywords:\n",
					"            if len(kw) <= 2:\n",
					"                pattern = r'\\b' + re.escape(kw) + r'\\b'\n",
					"                if re.search(pattern, text):\n",
					"                    found.append(kw)\n",
					"            else:\n",
					"                pattern = r'\\b' + re.escape(kw.lower()) + r'\\b'\n",
					"                if re.search(pattern, text.lower()):\n",
					"                    found.append(kw)\n",
					"    return found\n",
					"\n",
					"extract_tools_spark = udf(extract_tools_udf, ArrayType(StringType()))    "
				],
				"execution_count": 30
			},
			{
				"cell_type": "code",
				"source": [
					"JOB_MAPPING = {\n",
					"    \"dataanalysis\": \"Data Analyst\",\n",
					"    \"data-analysis\": \"Data Analyst\",\n",
					"    \"datascience\": \"Data Scientist\",\n",
					"    \"data-science\": \"Data Scientist\",\n",
					"    \"dataengineering\": \"Data Engineer\",\n",
					"    \"data-engineering\": \"Data Engineer\",\n",
					"    \"businessanalysis\": \"Business Analyst\",\n",
					"    \"business-analysis\": \"Business Analyst\",\n",
					"    \"machinelearning\": \"Machine Learning Engineer\",\n",
					"    \"machine-learning\": \"Machine Learning Engineer\",\n",
					"    \"MachineLearning\": \"Machine Learning Engineer\",\n",
					"    \"Software Engineer\": \"Software Engineer\",\n",
					"    \"software engineer\": \"Software Engineer\",\n",
					"    \"Cloud Engineer\": \"Cloud Engineer\",\n",
					"    \"cloud-computing\": \"Cloud Engineer\"\n",
					"}\n",
					"\n",
					"def normalize_job(job):\n",
					"    if job is None:\n",
					"        return \"Unknown\"\n",
					"    key = job.lower().strip()\n",
					"    return JOB_MAPPING.get(key, job)\n",
					"\n",
					"normalize_job_udf = udf(normalize_job, StringType())\n",
					"\n",
					"JOB_KEYS = set(JOB_MAPPING.keys())              \n",
					"JOB_VALUES = set(JOB_MAPPING.values())   \n",
					""
				],
				"execution_count": 31
			},
			{
				"cell_type": "code",
				"source": [
					"def filter_blacklist_udf(tools):\n",
					"    return [tool for tool in tools if tool.lower() not in GITHUB_BLACKLIST]\n",
					"filter_udf = udf(filter_blacklist_udf, ArrayType(StringType()))    \n",
					"\n",
					"def filter_stack_udf(tools):\n",
					"    filtered = []\n",
					"    for t in tools:\n",
					"        if t:\n",
					"            low = t.lower().strip()\n",
					"            if low not in JOB_KEYS and low not in JOB_VALUES:\n",
					"                filtered.append(t)\n",
					"    return filtered\n",
					"\n",
					"filter_stack = udf(filter_stack_udf, ArrayType(StringType()))    "
				],
				"execution_count": 32
			},
			{
				"cell_type": "code",
				"source": [
					"spark = SparkSession.builder \\\n",
					"    .appName(\"SilverLayerProcessing\") \\\n",
					"    .getOrCreate()"
				],
				"execution_count": 33
			},
			{
				"cell_type": "code",
				"source": [
					"github_path = \"abfss://bronze@datalakeyassin.dfs.core.windows.net/github/*.json\"\n",
					"stackoverflow_path = \"abfss://bronze@datalakeyassin.dfs.core.windows.net/stackoverflow/*.json\"\n",
					"reddit_path = \"abfss://bronze@datalakeyassin.dfs.core.windows.net/reddit/*.json\"\n",
					"\n",
					"silver_base_path = \"abfss://silver@datalakeyassin.dfs.core.windows.net/\"\n",
					"\n",
					"\n",
					"df_github  = spark.read.option(\"multiline\", \"true\").json(github_path)\n",
					"df_stackoverflow  = spark.read.option(\"multiline\", \"true\").json(stackoverflow_path)\n",
					"df_reddit  = spark.read.option(\"multiline\", \"true\").json(reddit_path)"
				],
				"execution_count": 34
			},
			{
				"cell_type": "code",
				"source": [
					"control_file_path = silver_base_path + \"_control/processed_files.txt\"\n",
					"\n",
					"\n",
					"processed_files_df = spark.read.text(control_file_path)\n",
					"last_etl_time_str = processed_files_df.collect()[0][0].strip()\n",
					"last_etl_time = datetime.strptime(last_etl_time_str, \"%Y-%m-%d %H:%M:%S\")\n",
					"\n",
					"        \n",
					"df_github = df_github.withColumn(\"filename\", input_file_name())   \n",
					"df_stackoverflow = df_stackoverflow.withColumn(\"filename\", input_file_name())  \n",
					"df_reddit = df_reddit.withColumn(\"filename\", input_file_name())   "
				],
				"execution_count": 35
			},
			{
				"cell_type": "code",
				"source": [
					"def extract_ts(filename):\n",
					"    match = re.search(r'(\\d{4}-\\d{2}-\\d{2}_\\d{2}-\\d{2}-\\d{2})', filename)\n",
					"    if match:\n",
					"        ts_str = match.group(1)\n",
					"        return datetime.strptime(ts_str, \"%Y-%m-%d_%H-%M-%S\")\n",
					"    return None\n",
					"\n",
					"extract_ts_udf = udf(extract_ts, TimestampType())"
				],
				"execution_count": 36
			},
			{
				"cell_type": "code",
				"source": [
					"df_github = df_github.withColumn(\"ingestion_time\", extract_ts_udf(col(\"filename\")))\\\n",
					"                   .drop(\"filename\")\n",
					"df_stackoverflow = df_stackoverflow.withColumn(\"ingestion_time\", extract_ts_udf(col(\"filename\")))\\\n",
					"                   .drop(\"filename\")\n",
					"df_reddit = df_reddit.withColumn(\"ingestion_time\", extract_ts_udf(col(\"filename\")))\\\n",
					"                   .drop(\"filename\")"
				],
				"execution_count": 37
			},
			{
				"cell_type": "code",
				"source": [
					"df_github = df_github.filter(col(\"ingestion_time\") > last_etl_time)\n",
					"df_stackoverflow = df_stackoverflow.filter(col(\"ingestion_time\") > last_etl_time)\n",
					"df_reddit = df_reddit.filter(col(\"ingestion_time\") > last_etl_time)\n",
					""
				],
				"execution_count": 38
			},
			{
				"cell_type": "code",
				"source": [
					"df_github  = df_github.withColumn(\"source\", F.lit(\"github\"))\n",
					"df_reddit  = df_reddit.withColumn(\"source\", F.lit(\"reddit\"))\n",
					"df_stackoverflow   = df_stackoverflow.withColumn(\"source\", F.lit(\"stackoverflow\"))\n",
					""
				],
				"execution_count": 39
			},
			{
				"cell_type": "code",
				"source": [
					"df_github.show(5)\n",
					"df_stackoverflow.show(5)\n",
					"df_reddit.show(5)\n",
					""
				],
				"execution_count": 40
			},
			{
				"cell_type": "code",
				"source": [
					"df_github = df_github.withColumn(\"job\", normalize_job_udf(\"job\"))\n",
					"df_reddit = df_reddit.withColumn(\"job\", normalize_job_udf(\"job\"))\n",
					"df_stackoverflow = df_stackoverflow.withColumn(\"job\", normalize_job_udf(\"job\"))"
				],
				"execution_count": 41
			},
			{
				"cell_type": "code",
				"source": [
					"df_reddit.select(\"job\").distinct().show()\n",
					""
				],
				"execution_count": 42
			},
			{
				"cell_type": "code",
				"source": [
					"df_stackoverflow = df_stackoverflow.withColumn(\"date_converted\", from_unixtime(col(\"date\"))) \\\n",
					"                   .withColumn(\"year\", year(col(\"date_converted\"))) \\\n",
					"                   .withColumn(\"month\", month(col(\"date_converted\"))) \\\n",
					"                   .withColumn(\"day\", dayofmonth(col(\"date_converted\"))) \\\n",
					"                   .drop(\"date_converted\")\\\n",
					"                   .drop(\"date\")\n",
					"\n",
					"# Reddit\n",
					"df_reddit = df_reddit.withColumn(\"date_converted\", from_unixtime(col(\"date\"))) \\\n",
					"                     .withColumn(\"year\", year(col(\"date_converted\"))) \\\n",
					"                     .withColumn(\"month\", month(col(\"date_converted\"))) \\\n",
					"                     .withColumn(\"day\", dayofmonth(col(\"date_converted\"))) \\\n",
					"                     .drop(\"date_converted\")\\\n",
					"                     .drop(\"date\")\n",
					"\n",
					"# GitHub\n",
					"df_github = df_github.withColumn(\"created_at_dt\", col(\"created_at\").cast(\"timestamp\")) \\\n",
					"                     .withColumn(\"year\", year(col(\"created_at_dt\"))) \\\n",
					"                     .withColumn(\"month\", month(col(\"created_at_dt\"))) \\\n",
					"                     .withColumn(\"day\", dayofmonth(col(\"created_at_dt\"))) \\\n",
					"                     .drop(\"created_at_dt\")\\\n",
					"                     .drop(\"created_at\")"
				],
				"execution_count": 43
			},
			{
				"cell_type": "code",
				"source": [
					"df_github.select(\"year\").distinct().show(5)\n",
					""
				],
				"execution_count": 44
			},
			{
				"cell_type": "code",
				"source": [
					"df_reddit = df_reddit.withColumn(\"text_to_parse\", concat_ws(\" \", col(\"title\"), col(\"selftext\")))\\\n",
					"                      .drop(\"title\")\\\n",
					"                      .drop(\"selftext\")\n",
					"                      \n",
					"df_github = df_github.withColumn(\"text_to_parse\", concat_ws(\" \", col(\"description\"), col(\"readme\")))\\\n",
					"                      .drop(\"description\")\\\n",
					"                      .drop(\"readme\")\n",
					"df_stackoverflow = df_stackoverflow.withColumn(\"text_to_parse\", concat_ws(\" \", col(\"title\"), col(\"body\")))\\\n",
					"                      .drop(\"title\")\\\n",
					"                      .drop(\"body\")"
				],
				"execution_count": 45
			},
			{
				"cell_type": "code",
				"metadata": {
					"jupyter": {
						"outputs_hidden": true
					}
				},
				"source": [
					"df_github = df_github.withColumn(\"tools\", extract_tools_spark(col(\"text_to_parse\")))\n",
					"df_github = df_github.withColumn(\"tools\", filter_udf(col(\"tools\")))\n",
					"df_reddit=df_reddit.withColumn(\"tools\", extract_tools_spark(col(\"text_to_parse\")))\n",
					"df_stackoverflow=df_stackoverflow.withColumn(\"tools\", filter_stack(col(\"tools\")))\n",
					"\n",
					""
				],
				"execution_count": 46
			},
			{
				"cell_type": "code",
				"source": [
					"df_github = df_github.drop(\"text_to_parse\")\\\n",
					"                      .drop(\"url\")\\\n",
					"                      .drop(\"language\")\n",
					""
				],
				"execution_count": 47
			},
			{
				"cell_type": "code",
				"source": [
					"df_github.show(5)\n",
					"df_reddit.show(5)\n",
					"df_stackoverflow.show(5)"
				],
				"execution_count": 48
			},
			{
				"cell_type": "code",
				"source": [
					"\n",
					"\n",
					"github_silver_path = silver_base_path + \"github/\"\n",
					"stackoverflow_silver_path = silver_base_path + \"stackoverflow/\"\n",
					"reddit_silver_path = silver_base_path + \"reddit/\"\n",
					"\n",
					"df_github.write.mode(\"append\").partitionBy(\"year\", \"month\").parquet(github_silver_path)\n",
					"df_stackoverflow.write.mode(\"append\").partitionBy(\"year\", \"month\").parquet(stackoverflow_silver_path)\n",
					"df_reddit.write.mode(\"append\").partitionBy(\"year\", \"month\").parquet(reddit_silver_path)"
				],
				"execution_count": 52
			},
			{
				"cell_type": "code",
				"source": [
					"# current_etl_time = datetime.utcnow()\n",
					"# with open(control_file_path, \"w\") as f:\n",
					"#     f.write(current_etl_time.strftime(\"%Y-%m-%d %H:%M:%S\"))"
				],
				"execution_count": 50
			}
		]
	}
}