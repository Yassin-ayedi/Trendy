{
	"name": "Gold",
	"properties": {
		"nbformat": 4,
		"nbformat_minor": 2,
		"bigDataPool": {
			"referenceName": "SilverSpark",
			"type": "BigDataPoolReference"
		},
		"sessionProperties": {
			"driverMemory": "28g",
			"driverCores": 4,
			"executorMemory": "28g",
			"executorCores": 4,
			"numExecutors": 2,
			"runAsWorkspaceSystemIdentity": false,
			"conf": {
				"spark.dynamicAllocation.enabled": "false",
				"spark.dynamicAllocation.minExecutors": "2",
				"spark.dynamicAllocation.maxExecutors": "2",
				"spark.autotune.trackingId": "78bb892a-d68a-4789-82a0-fa419354c961"
			}
		},
		"metadata": {
			"saveOutput": true,
			"enableDebugMode": false,
			"language_info": {
				"name": "python"
			},
			"a365ComputeOptions": {
				"id": "/subscriptions/7ce1bb29-7a9b-473d-a49e-a335cd2f0e33/resourceGroups/datawarehouse-rg/providers/Microsoft.Synapse/workspaces/my-synapse/bigDataPools/SilverSpark",
				"name": "SilverSpark",
				"type": "Spark",
				"endpoint": "https://my-synapse.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/SilverSpark",
				"auth": {
					"type": "AAD",
					"authResource": "https://dev.azuresynapse.net",
					"authHeader": null
				},
				"sparkVersion": "3.5",
				"nodeCount": 3,
				"cores": 4,
				"memory": 28,
				"extraHeader": null
			},
			"sessionKeepAliveTimeout": 30
		},
		"cells": [
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql import SparkSession\n",
					"from pyspark.sql.functions import col, udf, year, month, dayofmonth, to_timestamp,from_unixtime,concat_ws,explode,current_timestamp\n",
					"from pyspark.sql.types import ArrayType, StringType\n",
					"from pyspark.sql import functions as F\n",
					"from datetime import datetime\n",
					"import re"
				],
				"execution_count": 6
			},
			{
				"cell_type": "code",
				"source": [
					"silver_base = \"abfss://silver@datalakeyassin.dfs.core.windows.net/\"\n",
					"control_file_path = silver_base + \"_control/processed_files.txt\"\n",
					"\n",
					"df_github_silver = spark.read.parquet(silver_base + \"github/\")\n",
					"df_stack_silver  = spark.read.parquet(silver_base + \"stackoverflow/\")\n",
					"df_reddit_silver = spark.read.parquet(silver_base + \"reddit/\")\n",
					""
				],
				"execution_count": 3
			},
			{
				"cell_type": "code",
				"source": [
					"processed_files_df = spark.read.text(control_file_path)\n",
					"last_etl_time_str = processed_files_df.collect()[0][0].strip()\n",
					"last_etl_time = datetime.strptime(last_etl_time_str, \"%Y-%m-%d %H:%M:%S\")"
				],
				"execution_count": 28
			},
			{
				"cell_type": "code",
				"source": [
					"df_github_silver = df_github_silver.filter(col(\"ingestion_time\") > last_etl_time)\n",
					"df_stack_silver = df_stack_silver.filter(col(\"ingestion_time\") > last_etl_time)\n",
					"df_reddit_silver = df_reddit_silver.filter(col(\"ingestion_time\") > last_etl_time)"
				],
				"execution_count": 29
			},
			{
				"cell_type": "code",
				"source": [
					"df_reddit_silver.show(5)\n",
					"df_stack_silver.show(5)"
				],
				"execution_count": 10
			},
			{
				"cell_type": "code",
				"source": [
					"df_github_gold = df_github_silver.withColumn(\n",
					"    \"engagement_raw\",\n",
					"    F.col(\"stars\") + 2*F.col(\"forks\") + F.col(\"watchers\")\n",
					")\n",
					"\n",
					"\n",
					"df_reddit_gold = df_reddit_silver.withColumn(\n",
					"    \"engagement_raw\",\n",
					"    F.col(\"score\") +\n",
					"    2*F.col(\"num_comments\") +\n",
					"    3*F.col(\"total_awards\")\n",
					")\n",
					"\n",
					"df_stack_gold = df_stack_silver.withColumn(\n",
					"    \"engagement_raw\",\n",
					"    F.col(\"score\")*0.5 +\n",
					"    F.col(\"answer_count\")*5 +\n",
					"    F.col(\"favorite_count\")*3 +\n",
					"    F.col(\"view_count\")/1000 \n",
					")"
				],
				"execution_count": 11
			},
			{
				"cell_type": "code",
				"source": [
					"df_github_gold.select(\"engagement_raw\").show(5)\n",
					"df_reddit_gold.select(\"engagement_raw\").show(5)\n",
					"df_stack_gold.select(\"engagement_raw\").show(5)"
				],
				"execution_count": 12
			},
			{
				"cell_type": "code",
				"source": [
					"def normalize(df):\n",
					"    stats = df.agg(\n",
					"        F.min(\"engagement_raw\").alias(\"min_eng\"),\n",
					"        F.max(\"engagement_raw\").alias(\"max_eng\")\n",
					"    ).collect()[0]\n",
					"\n",
					"    min_eng = stats[\"min_eng\"]\n",
					"    max_eng = stats[\"max_eng\"]\n",
					"\n",
					"    return df.withColumn(\n",
					"        \"engagement\",\n",
					"        (F.col(\"engagement_raw\") - min_eng) / (max_eng - min_eng)\n",
					"    )\n",
					"df_github_gold = normalize(df_github_gold)\n",
					"df_reddit_gold = normalize(df_reddit_gold)\n",
					"df_stack_gold = normalize(df_stack_gold)\n",
					"\n",
					""
				],
				"execution_count": 13
			},
			{
				"cell_type": "code",
				"source": [
					"cols = [\"job\", \"tools\", \"year\", \"month\", \"day\", \"engagement\",\"source\",\"ingestion_time\"]\n",
					"\n",
					"df_github_gold = df_github_gold.select(cols)\n",
					"df_reddit_gold = df_reddit_gold.select(cols)\n",
					"df_stack_gold = df_stack_gold.select(cols)"
				],
				"execution_count": 15
			},
			{
				"cell_type": "code",
				"source": [
					"df_github_gold.show(5)\n",
					"df_reddit_gold.show(5)\n",
					"df_stack_gold.show(5)"
				],
				"execution_count": 16
			},
			{
				"cell_type": "code",
				"source": [
					"from pyspark.sql import functions as F\n",
					"df = df_github_gold.unionByName(df_reddit_gold).unionByName(df_stack_gold)\n",
					"\n",
					"staging_social = (\n",
					"    df\n",
					"    .withColumn(\"tool\", F.explode(\"tools\")) \n",
					"    .groupBy(\"job\", \"tool\", \"year\", \"month\", \"day\", \"source\")\n",
					"    .agg(\n",
					"        F.sum(\"engagement\").alias(\"job_engagement\"), \n",
					"        F.count(\"*\").alias(\"tool_mentions\"),\n",
					"        F.max(\"ingestion_time\").alias(\"ingestion_time\")       \n",
					"    )\n",
					")\n",
					"\n",
					"staging_social.show(20)\n",
					"\n",
					""
				],
				"execution_count": 22
			},
			{
				"cell_type": "code",
				"source": [
					"staging_social.write.mode(\"append\").parquet(\n",
					"    \"abfss://gold@datalakeyassin.dfs.core.windows.net/social_popularity/\"\n",
					")"
				],
				"execution_count": 23
			},
			{
				"cell_type": "code",
				"source": [
					"current_etl_time = datetime.utcnow()\n",
					"df_etl_time = spark.createDataFrame(\n",
					"    [(current_etl_time.strftime(\"%Y-%m-%d %H:%M:%S\"),)]\n",
					")\n",
					"\n",
					"# Overwrite the control file in ADLS\n",
					"df_etl_time.coalesce(1).write.mode(\"overwrite\").text(control_file_path)"
				],
				"execution_count": 25
			},
			{
				"cell_type": "code",
				"source": [
					""
				]
			}
		]
	}
}