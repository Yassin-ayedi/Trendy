# Trendy Jobs & Tools â€“ Azure Data Warehouse

## ðŸ“Œ Project Overview
This project implements a **modern data warehouse on Azure Cloud** to analyze **trending jobs and tools** based on both **real-time popularity** and **historical market demand**.

The goal is to understand:
- What jobs and tools are **popular right now**
- How their **demand evolved over past years**
- How popularity aligns (or not) with market demand

---

## ðŸ§  Data Sources
The warehouse ingests data from multiple sources:

### Real-time / Near real-time popularity
- **Reddit**
- **Stack Overflow**
- (Other social / community platforms can be added later)

These sources are used to measure **popularity** based on activity and mentions.

### Historical demand
- CSV databases containing **past years market data**
- Used to measure **job and tool demand over time**

---

## ðŸ“Š Data Model
The warehouse is built around **two main fact tables**:
- **Popularity fact** â†’ real-time / near real-time metrics
- **Demand fact** â†’ historical market demand

This allows comparison between:
- *What is trendy now*
- *What is actually demanded in the market*

---

## â˜ï¸ Cloud Architecture
The solution is built using a **modern Azure data architecture** with **minimal infrastructure requirements**:

- **Azure Data Lake Storage Gen2**
- **Azure Synapse Analytics**
  - PySpark notebooks
  - SQL scripts
  - Pipelines

### Data Lake Zones
The Data Lake is organized into three containers following best practices:

- **Bronze** â†’ raw, unprocessed data
- **Silver** â†’ cleaned and transformed data
- **Gold** â†’ analytics-ready data (facts & dimensions)

---

## ðŸ“ Repository Structure
.
â”œâ”€â”€ extract/
â”‚ â””â”€â”€ Local extraction scripts (VS Code)
â”‚ - Fetch data from external sources
â”‚ - Upload raw data directly into the Bronze layer
â”‚ - Uses Azure extensions & credentials
â”‚
â”œâ”€â”€ transform/
â”‚ â””â”€â”€ Azure Synapse PySpark notebooks
â”‚ - Data cleaning
â”‚ - Data transformation
â”‚ - Bronze â†’ Silver â†’ Gold
â”‚
â”œâ”€â”€ load(sql)/
â”‚ â””â”€â”€ SQL scripts & notebooks (Azure Synapse)
â”‚ - Fact table loading
â”‚ - Aggregations
â”‚ - Warehouse modeling
â”‚
â”œâ”€â”€ synapse_publish/
â”‚ â””â”€â”€ Auto-generated Azure Synapse artifacts
â”‚ - Published notebooks
â”‚ - Pipelines
â”‚ - Queries
â”‚ - Workspace configuration
â”‚
â””â”€â”€ README.md

---

## âš™ï¸ Configuration (to be added)
Configuration files (connections, secrets, parameters) will be added later.

Planned:
- Environment-specific configuration
- Secure credentials (Key Vault)
- Parameterized paths and resources

> ðŸ“Œ **Placeholder**: Configuration documentation will be added here.

---

## ðŸš€ How the Pipeline Works (High Level)
1. **Extract**
   - Run locally using VS Code
   - Fetch data from Reddit, Stack Overflow, and historical CSVs
   - Store raw data in **Bronze**

2. **Transform**
   - PySpark notebooks in Azure Synapse
   - Clean and enrich data
   - Move data to **Silver** and **Gold**

3. **Load**
   - SQL scripts build facts and analytics tables
   - Support reporting and trend analysis

---

## ðŸ›  Technologies Used
- Azure Data Lake Storage Gen2
- Azure Synapse Analytics
- PySpark
- SQL
- Python
- GitHub

---

## ðŸ“Œ Notes
- Raw data is **not versioned** in GitHub
- GitHub contains **code only**
- Data lives exclusively in the Data Lake

---






The GitHub repository is organized as follows:

